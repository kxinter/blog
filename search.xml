<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Openstack热迁移和冷迁移]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F28%2FOpenstack%E7%83%AD%E8%BF%81%E7%A7%BB%E5%92%8C%E5%86%B7%E8%BF%81%E7%A7%BB%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Dokcer常用命令]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F28%2FDokcer%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1 Docker常用命令1.1 镜像管理12345678910# 列出本地所有镜像docker images# 查找imagedocker search &lt;IMAGE_ID/NAME&gt;# 下载imagedocker pull &lt;IMAGE_ID&gt;# 上传imagedocker push &lt;IMAGE_ID&gt;# 删除imagedocker rmi &lt;IMAGE_ID&gt; 1.2 容器管理123456789101112131415161718192021222324252627282930313233343536373839404142434445docker run -i -t &lt;IMAGE_ID&gt; /bin/bash： -i：标准输入给容器 -t：分配一个虚拟终端 /bin/bash：执行bash脚本-d：以守护进程方式运行（后台）-P：默认匹配docker容器的5000端口号到宿主机的49153 to 65535端口-p &lt;HOT_PORT&gt;:&lt;CONTAINER_PORT&gt;：指定端口号--name： 指定容器的名称--rm：退出时删除容器# 停止containerdocker stop &lt;CONTAINER_ID&gt;#重新启动containerdocker start &lt;CONTAINER_ID&gt;#显示运行的容器docker ps-l：显示最后启动的容器-a：同时显示停止的容器，默认只显示启动状态# 连接到启动的容器docker attach &lt;CONTAINER_ID&gt;#输出容器日志docker logs &lt;CONTAINER_ID&gt;-f：实时输出# 复制容器内的文件到宿主机目录上docker cp &lt;CONTAINER_ID&gt;:路径 宿主机路径# 删除containerdocker rm &lt;CONTAINER_ID&gt;# 删除所有容器docker rm `docker ps -a -q`docker kill `docker ps -q`docker rmi `docker images -q -a`docker wait &lt;CONTAINER_ID&gt;：阻塞对容器的其他调用方法，直到容器停止后退出# 查看容器中运行的进程docker top &lt;CONTAINER_ID&gt;# 查看容器中的变化docker diff &lt;CONTAINER_ID&gt;# 查看容器详细信息（输出为Json）docker inspect &lt;CONTAINER_ID&gt;-f：查找特定信息，如docker inspect -f '&#123;&#123; .NetworkSettings.IPAddress &#125;&#125;'docker commit -m "comment" -a "author" &lt;CONTAINER_ID&gt; ouruser/imagename:tagdocker extc -it &lt;CONTAINER&gt; &lt;COMMAND&gt;：在容器里执行命令，并输出结果 1.3 网络管理1234567891011121314# 随机分配端口号docker run -P# 绑定特定端口号（主机的所有网络接口的5000端口均绑定容器的5000端口）docker run -p 5000:5000# 绑定主机的特定接口的端口号docker run -p 127.0.0.1:5000:5000# 绑定udp端口号docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py# 查看容器的5000端口对应本地机器的IP和端口号docker port &lt;CONTAINER_ID&gt; 5000# 使用Docker Linking连接容器：# Docker为源容器和接收容器创建一个安全的通道，容器之间不需要暴露端口，接收的容器可以访问源容器的数据docker run -d -P --name &lt;CONTAINER_NAME&gt; --link &lt;CONTAINER_NAME_TO_LINK&gt;:&lt;ALIAS&gt; 1.4 数据管理1234567891011121314151617181920212223# Data Volumes：volume是在一个或多个容器里指定的特殊目录# 数据卷可以在容器间共享和重复使用# 可以直接修改容器卷的数据# 容器卷里的数据不会被包含到镜像中# 容器卷保持到没有容器再使用它# 可以在容器启动的时候添加-v参数指定容器卷，也可以在Dockerfile里用VOLUMN命令添加docker run -d -P --name web -v /webapp training/webapp python app.py# 也可以将容器卷挂载到宿主机目录或宿主机的文件上，&lt;容器目录或文件&gt;的内容会被替换为&lt;宿主机目录或文件&gt;的内容，默认容器对这个目录有可读写权限docker run -d -P --name web -v &lt;宿主机目录&gt;:&lt;容器目录&gt; training/webapp python app.py# 可以通过指定ro，将权限改为只读docker run -d -P --name web -v &lt;宿主机目录&gt;:&lt;容器目录&gt;:ro training/webapp python app.py# 在一个容器创建容器卷后，其他容器便可以通过--volumes-from共享这个容器卷数据，如下：docker run -d -v /dbdata --name db1 training/postgres echo Data-only container for postgres# 首先启动了一个容器，并为这个容器增加一个数据卷/dbdata，然后启动另一个容器，共享这个数据卷docker run -d --volumes-from db1 --name db2 training/postgres# 此时db2使用了db1的容器卷，当容器db1被删除时，容器卷也不会被删除，只有所有容器不再使用此容器卷时，才会被删除docker rm -v：删除容器卷# 除了共享数据外，容器卷另一个作用是用来备份、恢复和迁移数据docker run --volumes-from db1 -v /home/backup:/backup ubuntu tar cvf /backup/backup.tar /dbdata# 启动一个容器数据卷使用db1容器的数据卷，同时新建立一个数据卷指向宿主机目录/home/backup，将/dbdata目录的数据压缩为/backup/backup.tardocker run -v /dbdata --name dbdata2 ubuntu /bin/bashdocker run --volumes-from dbdata2 -v /home/backup:/backup busybox tar xvf /backup/backup.tar# 启动一个容器，同时把backup.tar的内容解压到容器的backup 1.5 仓库管理docker login：登录 1.6 镜像打包及恢复1.6.1 保存镜像为文件1234# centos为镜像，centos为打包名，centos后可以指定标签docker save -o centos.tar centosordocker save -o centos.tar centos:7.2 1.6.2 从文件载入镜像1234# home/centos.tar为镜像文件路径docker load --input /home/centos.tarordocker load &lt; /home/centos.tar]]></content>
      <categories>
        <category>Dokcer</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu部署本地源仓库]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F27%2FUbuntu%E9%83%A8%E7%BD%B2%E6%9C%AC%E5%9C%B0%E6%BA%90%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1 本地源的制作1.1 安装所需软件包1sudo apt-get install dpkg-dev 1.2 打包deb软件包12#加上-d参数，只下载安装包，不安装及解压。sudo apt-get install -d nginx 将/var/cache/apt/archives/下的所有deb文件拷到/home/packages/下的Natty目录中：/home/packages/Natty，拷贝前建议执行一下: 12#autoclean - 删除已下载的旧包文件sudo apt-get autoclean 1.3 进入指定目录上一级目录拷完后在终端中进入刚才新建的目录Natty所在的上一级目录，也就是： 1cd /home/packages 生成软件包依赖信息文件 1sudo dpkg-scanpackages Natty/ | gzip &gt;Natty/Packages.gz 至此本地源的软件包已经准备完毕；下面接着介绍如何使用。 2 本地源的使用2.1 本机源服务器的搭建将地址加入更新源列表文件 123$ sudo vim /etc/apt/sources.list# 添加以下路径，其它deb信息使用#号注释掉deb file:///home/packages/ Natty/ #注意Natty后面有一个斜杠,前面还要有空格(这是书写方式) 2.2 更新源信息12#更新信息，生成数据缓存$ sudo apt-get update 之后即可正常安装所需软件。 3 局域网源服务器3.1 安装apache21sudo apt-get install apache2 启动服务 注意：配置apache2的时候注意端口，不要配置成可能被其他网络应用使用的端口就可以。 3.2 配置服务器上的Ubuntu源12#在apache2发布目录/var/www/html位置创建到源目录的软链接sudo ln -s /home/packages/Natty/ /var/www/html/ubuntu-local 3.3 配置局域网客户机sources.list1234$ sudo vim /etc/apt/sources.list#添加如下信息,其它deb信息使用#号注释掉deb http://192.168.1.224 ubuntu-local/ #注意书写方式，ip地址后空格，目录地址/ 3.4 客户机更新源信息12#更新信息，生成数据缓存sudo apt-get update 之后即可正常安装所需软件。 4 参考资料https://www.iyunv.com/thread-384273-1-1.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>私有仓库</tag>
        <tag>部署</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu同步源仓库到本地]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F27%2FUbuntu%E5%90%8C%E6%AD%A5%E6%BA%90%E4%BB%93%E5%BA%93%E5%88%B0%E6%9C%AC%E5%9C%B0%2F</url>
    <content type="text"><![CDATA[1 步骤1.1 安装apt-mirror1sudo apt-get install apt-mirro 1.2 创建下载目录12345#路径自定义.../ubuntu.../ubuntu/mirror.../ubuntu/skel.../ubuntu/var 1.3 修改配置文件1sudo vi /etc/apt/mirror.list 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263############# config ###################set base_path /home/packages/ubuntu#set mirror_path $base_path/mirrorset skel_path $base_path/skelset var_path $base_path/varset cleanscript $var_path/clean.shset nthreads 20set _tilde 0############## end config ###############deb http://archive.ubuntu.com/ubuntu trusty main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-security main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-updates main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-proposed main restricted universe multiverse#deb http://archive.ubuntu.com/ubuntu trusty-backports main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-security main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-updates main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-proposed main restricted universe multiverse#deb-src http://archive.ubuntu.com/ubuntu trusty-backports main restricted universe multiverse#clean http://archive.ubuntu.com/ubuntu#自定义#参考以下配置文件：#清空原有的配置文件，直接使用以下配置文件即可############# config ################### 以下注释的内容都是默认配置，如果需要自定义，取消注释修改即可#set base_path /var/spool/apt-mirror## 镜像文件下载地址# set mirror_path $base_path/mirror# 临时索引下载文件目录，也就是存放软件仓库的dists目录下的文件（默认即可）# set skel_path $base_path/skel# 配置日志（默认即可）# set var_path $base_path/var# clean脚本位置# set cleanscript $var_path/clean.sh# 架构配置，i386/amd64，默认的话会下载跟本机相同的架构的源#set defaultarch amd64# set postmirror_script $var_path/postmirror.sh# set run_postmirror 0# 下载线程数#set nthreads 20#set _tilde 0############## end config ############### Ali yun（这里没有添加deb-src的源）#我们把常用的软件同步过来就够用了deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse#当某些软件包在服务器端进行了升级，或者服务器端不再需要这些软件包时，我们使用了 apt-mirror与服务器同步后，会在本地的$var_path/下生成一个clean.sh的脚本，列出了遗留在本地的旧版本和无用的软件包，你可 以手动运行这个脚本来删除遗留在本地的且不需要用的软件包clean http://mirrors.aliyun.com/ubuntu 如果用amd64位架构下的包，可以加上deb-amd64的标记如果什么都不加，直接使用deb http…..这种格式，则在同步时，只同步当前系统所使用的架构下的软件包。比如一个64位系统，直接debhttp….只同步64位的软件 包。如果还嫌麻烦，直接去改set defaultarch 这个参数就好，比如改成set defaultarch i386，这样你使用debhttp…..这种格式，则在同步时，只同步i386的软件包了。 如果你还想要源码，可以把源码也加到mirror.list里面同步过来，比如加上deb-src这样的标记。想要其他的东西也可以追加相应的标记来完成。 1.4 同步源配置好后我们就可以和指定的镜像进行同步了 1sudo apt-mirror 1.5 建立本地源参考：Ubuntu部署本地源仓库 2 参考资料http://www.linuxidc.com/Linux/2014-08/105415.htm https://www.iyunv.com/thread-384273-1-1.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>私有仓库</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker集群-Docker_Swarm]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F27%2FDocker%E9%9B%86%E7%BE%A4-Docker-Swarm%2F</url>
    <content type="text"><![CDATA[1 安装 Swarm1.1 下载镜像1$ docker pull swarm 可以使用下面的命令来查看 Swarm 版本，验证是否成功下载 Swarm 镜像。 12$ docker run --rm swarm -vswarm version 1.2.2 (34e3da3) 1.2 配置节点Docker 主机在加入 Swarm 集群前，需要进行一些简单配置，添加 Docker daemon 的网络监听。 例如，在启动 Docker daemon 的时候通过 -H 参数： 1$ sudo docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 注：Docker 1.8.0 版本之前不支持 daemon 命令，可以用 -d 代替。 如果是通过服务方式启动，则需要修改服务的配置文件。 以 Ubuntu 14.04 为例，配置文件为 /etc/default/docker（其他版本的 Linux 上略有不同）。 在文件的最后添加： 1DOCKER_OPTS="$DOCKER_OPTS -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock" 1.3 启动集群Docker 集群管理需要使用服务发现（Service Discover）功能，Swarm 支持以下的几种方式：DockerHub、本地文件、etcd、consul、zookeeper 和手动指定节点 IP 地址信息等。 除了手动指定外，这些方法原理上都是通过维护一套数据库机制，来管理集群中注册节点的 Docker daemon 的访问信息。 本地配置集群推荐使用 consul 作为服务发现后端。利用社区提供的 Docker 镜像，整个过程只需要三步即可完成。 1.3.1 启动 Consul 服务后端启动 consul 服务容器，映射到主机的 8500 端口。 1$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap 获取到本地主机的地址作为 consul 的服务地址：&lt;consul_ip&gt;:8500。 1.3.2 启动管理节点首先，启动一个主管理节点，映射到主机的 4000 端口，并获取所在主机地址为 &lt;manager0_ip&gt;。其中 4000 端口是 Swarm 管理器的默认监听端口，用户也可以指定映射为其它端口。 1$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &lt;manager0_ip&gt;:4000 consul://&lt;consul_ip&gt;:8500 为了提高高可用性，用户也可以启动从管理节点。假定获取所在主机地址为 &lt;manager1_ip&gt;。 1$ docker run -d swarm manage -H :4000 --replication --advertise &lt;manager1_ip&gt;:4000 consul://&lt;consul_ip&gt;:8500 1.3.3 启动工作节点需要在每个工作节点上启动 agent 服务。 获取节点的主机地址为 &lt;node_ip&gt;，并指定前面获取到的 consul 服务地址。 1$ docker run -d swarm join --advertise=&lt;node_ip&gt;:2375 consul://&lt;consul_ip&gt;:8500 节点启动后，用户可以指定 Docker 服务地址为 &lt;manager0_ip&gt;:4000&gt; 来测试各种 Docker 命令，可以看到整个 Swarm 集群就像一个虚拟的 Docker 主机一样正常工作。 由于 Swarm 实际上是通过 agent 调用了本地的 Docker daemon 来运行容器，当 Swarm 集群服务出现故障时，无法接受新的请求，但已经运行起来的容器将不会受到影响。 2 使用 Swarm前面演示了基于 consul 服务发现后端来配置一个本地 Swarm 集群。其中，consul 也可以被替换为 etcd、zookeeper 等。 另外一个更方便的方式是直接使用 DockerHub 提供的免费服务发现后端。 下面使用这种方式来演示 Swarm 的主要操作，包括： create：创建一个集群； list：列出集群中的节点； manage：管理一个集群； join：让节点加入到某个集群。 注意，使用 DockerHub 的服务发现后端，需要各个节点能通过公网访问到 DockerHub 的服务接口。 2.1 创建集群 id在任意一台安装了 Swarm 的机器上执行 swarm create 命令来在 DockerHub 服务上进行注册。 Swarm 会通过服务发现后端（此处为 DockerHub 提供）来获取一个唯一的由数字和字母组成的 token，用来标识要管理的集群。 12$ docker run --rm swarm create946d65606f7c2f49766e4dddac5b4365 注意返回的字符串，这是集群的唯一 id，加入集群的各个节点将需要这个信息。 2.2 配置集群节点在所有要加入集群的普通节点上面执行 swarm join 命令，表示把这台机器加入指定集群当中。 例如某台机器 IP 地址为 192.168.0.2，将其加入我们刚创建的 946d65606f7c2f49766e4dddac5b4365 集群，则可以通过： 12$ docker run --rm swarm join --addr=192.168.0.2:2375 token://946d65606f7c2f49766e4dddac5b4365time="2015-12-09T08:59:43Z" level=info msg="Registering on the discovery service every 20s..." addr="192.168.0.2:2375" discovery="token://946d65606f7c2f49766e4dddac5b4365" 注：其中 –addr 指定的 IP 地址信息将被发送给服务发现后端，用以区分集群不同的节点。manager服务必须要通过这个地址可以访问到该节点。 通过控制台可以看到，上述命令执行后，默认每隔 20 秒（可以通过 –heartbeat 选项指定），会输出一条心跳信息。对于发现服务后端来说，默认如果超过 60 秒（可以通过 –ttl 选项指定）没有收到心跳信息，则将节点从列表中删除。 如果不希望看到输出日志信息，则可以用 -d 选项替换 –rm 选项，让服务后台执行。 执行 swarm join 命令实际上是通过 agent 把自己的信息注册到发现服务上，因此，此时对于后端的发现服务来说，已经可以看到有若干节点注册上来了。那么，如何管理和使用这些节点呢，这就得需要 Swarm 的 manager 服务了。 2.3 配置管理节点配置管理节点需要通过 swarm manage 命令，该命令将启动 manager 服务，默认监听到 2375 端口，所有对集群的管理可以通过该服务接口进行。 读者可能注意到，manager 服务默认监听的端口跟 Docker 服务监听端口是一样的，这是为了兼容其它基于 Docker 的服务，可以无缝地切换到 Swarm 平台上来。 仍然在节点 192.168.0.2 进行操作。由于我们是采用 Docker 容器形式启动 manager 服务，本地的 2375端口已经被 Docker Daemon 占用。我们将 manager 服务监听端口映射到本地一个空闲的 12375 端口。 12$ docker run -d -p 12375:2375 swarm manage token://946d65606f7c2f49766e4dddac5b43651e1ca8c4117b6b7271efc693f9685b4e907d8dc95324350392b21e94b3cffd18 可以通过 docker ps 命令来查看启动的 swarm manager 服务容器。 123$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1e1ca8c4117b swarm "/swarm manage token:" 11 seconds ago Up 10 seconds 0.0.0.0:12375-&gt;2375/tcp jovial_rosalind 命令如果执行成功会返回刚启动的 Swarm 容器的 ID，此时一个简单的 Swarm 集群就已经搭建起来了，包括一个普通节点和一个管理节点。 2.4 查看集群节点列表集群启动成功以后，用户可以在任何一台节点上使用 swarm list 命令查看集群中的节点列表。例如 12$ docker run --rm swarm list token://946d65606f7c2f49766e4dddac5b4365192.168.0.2:2375 显示正是之前用 swarm join 命令加入集群的节点的地址。 我们在另外一台节点 192.168.0.3 上同样使用 swarm join 命令新加入一个节点： 123$ docker run --rm swarm join --addr=192.168.0.3:2375 token://946d65606f7c2f49766e4dddac5b4365time="2015-12-10T02:05:34Z" level=info msg="Registering on the discovery service every 20s..." addr="192.168.0.3:2375" discovery="token://946d65606f7c2f49766e4dddac5b4365"... 再次使用 swarm list 命令查看集群中的节点列表信息，可以看到新加入的节点： 123$ docker run --rm swarm list token://946d65606f7c2f49766e4dddac5b4365192.168.0.3:2375192.168.0.2:2375 2.5 使用集群服务那么，怎么使用 Swarm 提供的服务呢？ 实际上，所有 Docker 客户端可以继续使用，只要指定使用 Swarm manager 服务的监听地址即可。 例如，manager 服务监听的地址为 192.168.0.2:12375，则可以通过指定 -H 192.168.0.2:12375 选项来继续使用 Docker 客户端，执行任意 Docker 命令，例如 ps、info、run 等等。 在任意节点上使用 docker run 来启动若干容器，例如 12$docker -H 192.168.0.2:12375:12375 run -d ubuntu ping 127.0.0.14c9bccbf86fb6e2243da58c1b15e9378fac362783a663426bbe7058eea84de46 使用 ps 命令查看集群中正在运行的容器。 12345$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4c9bccbf86fb ubuntu "ping 127.0.0.1" About a minute ago Up About a minute clever_wright730061a3801a registry:latest "docker-registry" 2 minutes ago Up 2 minutes 192.168.0.2:5000-&gt;5000/tcp Host-1/registry_registry_172d99f24a06f redis:3.0 "/entrypoint.sh redis" 2 minutes ago Up 2 minutes 6379/tcp Host-1/registry_redis_1,Host-1/registry_registry_1/redis,Host-1/registry_registry_1/redis_1,Host-1/registry_registry_1/registry_redis_1 输出结果中显示目前集群中正在运行的容器（注意不包括 Swarm manager 服务容器），可以在不同节点上使用 docker ps 查看本地容器，发现这些容器实际上可能运行在集群中多个节点上（被 Swarm 调度策略进行分配）。 使用 info 查看所有节点的信息。 1234567891011121314151617181920$ docker -H 192.168.0.2:12375 infoContainers: 18Images: 36Role: primaryStrategy: spreadFilters: health, port, dependency, affinity, constraintNodes: 2 Host-1: 192.168.0.2:2375 └ Containers: 15 └ Reserved CPUs: 0 / 4 └ Reserved Memory: 1 GiB / 4.053 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-43-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs Host-2: 192.168.0.3:2375 └ Containers: 3 └ Reserved CPUs: 0 / 8 └ Reserved Memory: 0 B / 16.46 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-30-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufsCPUs: 12Total Memory: 20.51 GiBName: 1e1ca8c4117b 结果输出显示这个集群目前只有两个节点，地址分别是 192.168.0.2 和 192.168.0.3。 类似的，也可以通过 Compose 模板来启动多个服务。不过请注意，要想让服务分布到多个 Swarm 节点上，需要采用版本 2 的写法。 2.6 使用网络Swarm 为了支持跨主机的网络，默认采用了 overlay 网络类型，实现上通过 vxlan 来构建联通整个 Swarm 集群的网络。 首先，在集群中所有节点上，添加配置 Docker daemon 选项： 1--cluster-store=&lt;DISCOVERY_HOST:PORT&gt; --cluster-advertise=&lt;DOCKER_DAEMON_HOST:PORT&gt; 以 consul 服务为例，可能类似： 1--cluster-store=consul://&lt;consul 服务地址&gt;:8500 --cluster-advertise=192.168.0.3:2375 之后重启 Docker 服务。 首先，创建一个网络。 1$ docker -H 192.168.0.2:12375 network create swarm_network 查看网络，将看到一个 overlay 类型的网络。 123$ docker -H 192.168.0.2:12375 network lsNETWORK ID NAME DRIVER6edf2d16ec97 swarm_network overlay 此时，所有添加到这个网络上的容器将自动被分配到集群中的节点上，并且彼此联通。 3 使用其它服务发现后端Swarm 目前可以支持多种服务发现后端，这些后端功能上都是一致的，即维护属于某个集群的节点的信息。不同方案并无优劣之分，在实际使用时候，可以结合自身需求和环境限制进行选择，甚至自己定制其它方案。 使用中可以通过不同的路径来选择特定的服务发现后端机制。 -token://&lt;token&gt;：使用 DockerHub 提供的服务，适用于可以访问公网情况； file://path/to/file：使用本地文件，需要手动管理； consul://&lt;ip&gt;/&lt;path&gt;：使用 consul服务，私有环境推荐； etcd://&lt;ip1&gt;,&lt;ip2&gt;/&lt;path&gt;：使用 etcd 服务，私有环境推荐； zk://&lt;ip1&gt;,&lt;ip2&gt;/&lt;path&gt;：使用 zookeeper 服务，私有环境推荐； [nodes://]&lt;ip1&gt;,&lt;ip2&gt;：手动指定集群中节点的地址，方便进行服务测试。 3.1 使用文件使用本地文件的方式十分简单，就是讲所有属于某个集群的节点的 Docker daemon 信息写入一个文件中，然后让 manager 从这个文件中直接读取相关信息。 首先，在 Swarm 管理节点（192.168.0.2）上新建一个文件，把要加入集群的机器的 Docker daemon 信息写入文件： 1234$ tee /tmp/cluster_info &lt;&lt;-'EOF'192.168.0.2:2375192.168.0.3:2375EOF 然后，本地执行 swarm manage 命令，并指定服务发现机制为本地文件，注意因为是容器方式运行 manager，需要将本地文件挂载到容器内。 1$ docker run -d -p 12375:2375 -v /tmp/cluster_info:/tmp/cluster_info swarm manage file:///tmp/cluster_info 接下来就可以通过使用 Swarm 服务来进行管理了，例如使用 info 查看所有节点的信息。 1234567891011121314151617181920$ docker -H 192.168.0.2:12375 infoContainers: 18Images: 36Role: primaryStrategy: spreadFilters: health, port, dependency, affinity, constraintNodes: 2 Host-1: 192.168.0.2:2375 └ Containers: 15 └ Reserved CPUs: 0 / 4 └ Reserved Memory: 1 GiB / 4.053 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-43-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs Host-2: 192.168.0.3:2375 └ Containers: 3 └ Reserved CPUs: 0 / 8 └ Reserved Memory: 0 B / 16.46 GiB └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-30-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufsCPUs: 12Total Memory: 20.51 GiBName: e71eb5f1d48b 3.2 其它发现服务后端其它服务发现后端的使用方法，也是大同小异，不同之处在于使用 Swarm 命令时指定的路径格式不同。 例如，对于前面介绍的 consul 服务后端来说。 快速部署一个 consul 服务的命令为： 1$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap 之后创建 Swarm 的管理服务，指定使用 consul 服务，管理端口监听在本地的 4000 端口。 1$ docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &lt;manager_ip&gt;:4000 consul://&lt;consul_ip&gt;:8500 Swarm 节点注册时候命令格式类似于： 1$ swarm join --advertise=&lt;node_ip:2375&gt; consul://&lt;consul_addr&gt;/&lt;optional path prefix&gt; 对于 etcd 服务后端来说，节点注册时候命令格式类似于： 1$ swarm join --addr=&lt;node_addr:2375&gt; etcd://&lt;etcd_addr1&gt;,&lt;etcd_addr2&gt;/&lt;optional path prefix&gt; 启动管理服务时候，格式类似于： 1$ swarm manage -H tcp://&lt;manager_ip&gt;:4000 etcd://&lt;etcd_addr1&gt;,&lt;etcd_addr2&gt;/&lt;optional path prefix&gt; 3.3 地址和端口的范围匹配对于基于文件，以及手动指定节点信息两种服务发现后端机制来说，其中地址和端口域可以支持指定一个范围，以一次性指定多个地址。 例如： 192.168.0.[2:10]:2375 代表 192.168.0.2:2375 – 192.168.0.10:2375 一共 9 个地址； 192.168.0.2:[2:9]375 代表 192.168.0.2:2375 – 192.168.0.2:9375 一共 8 个地址。 4 Swarm 中的调度器调度是集群十分重要的功能，Swarm 目前支持三种调度策略：spread、binpack 和 random。 在执行swarm manage命令启动管理服务的时候，可以通过--strategy 参数指定调度策略，默认的是 spread。 简单来说，这三种调度策略的优化目标如下： spread：如果节点配置相同，选择一个正在运行的容器数量最少的那个节点，即尽量平摊容器到各个节点； binpack：跟 spread 相反，尽可能的把所有的容器放在一台节点上面运行，即尽量少用节点，避免容器碎片化。 random：直接随机分配，不考虑集群中节点的状态，方便进行测试使用。 4.1 spread 调度策略仍然以之前创建好的集群为例，来演示下 spread 策略的行为。 在 192.168.0.2 节点启动管理服务，管理 token://946d65606f7c2f49766e4dddac5b4365 的集群。 12$ docker run -d -p 12375:2375 swarm manage --strategy "spread" token://946d65606f7c2f49766e4dddac5b4365c6f25e6e6abbe45c8bcf75ac674f2b64d5f31a5c6070d64ba954a0309b197930 列出集群中节点。 123$ docker run --rm swarm list token://946d65606f7c2f49766e4dddac5b4365192.168.0.3:2375192.168.0.2:2375 此时，两个节点上除了 swarm 外都没有运行其它容器。 启动一个 ubuntu 容器。 12$ docker -H 192.168.0.2:12375 run -d ubuntu:14.04 ping 127.0.0.1bac3dfda5306181140fc959969d738549d607bc598390f57bdd432d86f16f069 查看发现它实际上被调度到了 192.168.0.3 节点（当节点配置相同时候，初始节点随机选择）。 再次启动一个 ubuntu 容器。 12$ docker -H 192.168.0.2:12375 run -d ubuntu:14.04 ping 127.0.0.18247067ba3a31e0cb692a8373405f95920a10389ce3c2a07091408281695281c 查看它的位置，发现被调度到了另外一个节点：192.168.0.2 节点。 1234$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES8247067ba3a3 ubuntu:14.04 "ping 127.0.0.1" 1 minutes ago Up 1 minutes Host-2/sick_galileobac3dfda5306 ubuntu:14.04 "ping 127.0.0.1" 2 minutes ago Up 2 minutes Host-3/compassionate_ritchie 当节点配置不同的时候，spread会更愿意分配到配置较高的节点上。 4.2 binpack 调度策略现在来看看 binpack 策略下的情况。 直接启动若干 ubuntu 容器，并查看它们的位置。 1234567$ docker -H 192.168.0.2:12375 run -d ubuntu:14.04 ping 127.0.0.1$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4c4f45eba866 ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/hopeful_brown5e650541233c ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/pensive_wright99c5a092530a ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/naughty_engelbart4ab392c26eb2 ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/thirsty_mclean 可以看到，所有的容器都是分布在同一个节点（192.168.0.3）上运行的。 5 Swarm 中的过滤器Swarm 的调度器可以按照指定调度策略自动分配容器到节点。但有些时候希望能对这些分配加以干预。比如说，让 IO 敏感的容器分配到安装了 SSD 的节点上；让计算敏感的容器分配到 CPU 核数多的机器上；让网络敏感的容器分配到高带宽的机房；让某些容器尽量放同一个节点……。 这可以通过过滤器（filter）来实现，目前支持 Constraint、Affinity、Port、Dependency、Health等五种过滤器。 5.1 Constraint 过滤器Constraint 过滤器是绑定到节点的键值对，相当于给节点添加标签。 可在启动 Docker 服务的时候指定，例如指定某个节点颜色为 red。 1$ sudo docker daemon --label color=red -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock 同样的，可以写在 Docker 服务的配置文件里面（以 Ubuntu 14.04 为例，是 /etc/default/docker）。 1DOCKER_OPTS="--label color=red -H 0.0.0.0:2375 -H unix:///var/run/docker.sock" 使用 Swarm 启动容器的时候，采用 -e constarint:key=value 的形式，可以过滤选择出匹配条件的节点。 例如，我们将 192.168.0.2 节点打上红色标签，192.168.0.3 节点打上绿色标签。 然后，分别启动两个容器，指定使用过滤器分别为红色和绿色。 1234$ docker -H 192.168.0.2:12375 run -d -e constraint:color==red ubuntu:14.04 ping 127.0.0.1252ffb48e64e9858c72241f5eedf6a3e4571b1ad926faf091db3e26672370f64$ docker -H 192.168.0.2:12375 run -d -e constraint:color==green ubuntu:14.04 ping 127.0.0.13d6f8d7af8583416b17061d038545240c9e5c3be7067935d3ef2fbddce4b8136 注：指定标签中间是两个等号 查看它们将被分配到指定节点上。 1234$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES252ffb48e64e ubuntu:14.04 "ping 127.0.0.1" 1 minutes ago Up 1 minutes Host-2/sick_galileo3d6f8d7af858 ubuntu:14.04 "ping 127.0.0.1" 2 minutes ago Up 2 minutes Host-3/compassionate_ritchie 另外，Docker 内置了一些常见的过滤器，包括 node、storagedriver、executiondriver、kernelversion、operatingsystem 等。这些值可以通过 docker info 命令查看。 例如，目前集群中各个节点的信息为： 1234567891011121314151617181920$ docker -H 192.168.0.2:12375 infoContainers: 5Images: 39Role: primaryStrategy: spreadFilters: health, port, dependency, affinity, constraintNodes: 2 Host-2: 192.168.0.2:2375 └ Containers: 4 └ Reserved CPUs: 0 / 4 └ Reserved Memory: 1 GiB / 4.053 GiB └ Labels: color=red, executiondriver=native-0.2, kernelversion=3.16.0-43-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs Host-3: 192.168.0.3:2375 └ Containers: 1 └ Reserved CPUs: 0 / 8 └ Reserved Memory: 0 B / 16.46 GiB └ Labels: color=green, executiondriver=native-0.2, kernelversion=3.16.0-30-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufsCPUs: 12Total Memory: 20.51 GiBName: 946d65606f7c 5.2 Affinity 过滤器Affinity 过滤器允许用户在启动一个容器的时候，让它分配到某个已有容器的节点上。 例如，下面我们将启动一个 nginx 容器，让它分配到已经运行某个 ubuntu 容器的节点上。 在 Constraint 过滤器的示例中，我们分别启动了两个 ubuntu 容器 sick_galileo 和 compassionate_ritchie，分别在 Host-2 和 Host-3 上。 现在启动一个 nginx 容器，让它跟容器 sick_galileo 放在一起，都放到 Host-2 节点上。可以通过 -e affinity:container==&lt;name or id&gt; 参数来实现。 1$ docker -H 192.168.0.2:12375 run -d -e affinity:container==sick_galileo nginx 然后启动一个 redis 容器，让它跟容器 compassionate_ritchie 放在一起，都放到 Host-3 节点上。 1$ docker -H 192.168.0.2:12375 run -d -e affinity:container==compassionate_ritchie redis 查看所有容器运行情况。 123456$ docker -H 192.168.0.2:12375 psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES0a32f15aa8ee redis "/entrypoint.sh redis" 2 seconds ago Up 1 seconds 6379/tcp Host-3/awesome_darwind2b9a53e67d5 nginx "nginx -g 'daemon off" 29 seconds ago Up 28 seconds 80/tcp, 443/tcp Host-2/fervent_wilson252ffb48e64e ubuntu:14.04 "ping 127.0.0.1" 2 minutes ago Up 2 minutes Host-2/sick_galileo3d6f8d7af858 ubuntu:14.04 "ping 127.0.0.1" 3 minutes ago Up 3 minutes Host-3/compassionate_ritchie 5.3 其它过滤器其它过滤器的使用方法也是大同小异，例如通过 affinity:image==&lt;name or id&gt; 来选择拥有指定镜像的节点；通过 -e affinity:label_name==value 来选择拥有指定标签的容器所允许的节点。 此外，当容器端口需要映射到宿主机指定端口号的时候，Swarm 也会自动分配容器到指定宿主机端口可用的节点。 当不同容器之间存在数据卷或链接依赖的时候，Swarm 会分配这些容器到同一个节点上。 6 本章小结本章笔者介绍了 Docker Swarm 的安装、使用和主要功能。 通过使用 Swarm，用户可以将若干 Docker 主机节点组成的集群当作一个大的虚拟 Docker 主机使用。并且，原先基于单机的 Docker 应用，可以无缝的迁移到 Swarm 上来。 实现这些功能的前提是服务自动发现能力。在现代分布式系统中，服务的自动发现、注册、更新等能力将成为系统的基本保障和重要基础。 在生产环境中，Swarm 的管理节点和发现服务后端要采用高可用性上的保护，可以采用集群模式。 值得一提的是，Swarm V2 功能已经被无缝嵌入到了 Docker 1.12+ 版本中，用户今后可以直接使用 Docker 命令来完成相关功能的配置，这将使得集群功能的管理更加简便。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>swarm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openstack云镜像制作-Centos7篇]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F27%2FOpenstack%E4%BA%91%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C-Centos7%E7%AF%87%2F</url>
    <content type="text"><![CDATA[一、制作步骤1、安装kvm 参考centos 7系统安装配置kvm软件步骤 1、创建虚拟硬盘大小10G 名称：centos7-dis.qcow2 2、安装系统 注意一：分区，分区的时候只给”/“ 根目录分一个区即可，其他都不要。格式ext4注意二：网络设置方面，确保你的网卡eth0是DHCP状态的，而且请务必勾上”auto connect”的对勾 2、进入虚拟机系统操作关于CentOS镜像制作需要注意以下几点： (1) 修改网络信息 /etc/sysconfig/network-scripts/ifcfg-eth0 （删掉mac信息)，如下： 12345TYPE=Ethernet DEVICE=eth0 ONBOOT=yes BOOTPROTO=dhcp NM_CONTROLLED=no (2) 删除已生成的网络设备规则，否则制作的镜像不能上网1$ rm -rf /etc/udev/rules.d/70-persistent-net.rules (3)增加一行到/etc/sysconfig/network 1NOZERCONF=yes (4)安装cloud-init（可选），cloud-init可以在开机时进行密钥注入以及修改hostname等，关于cloud-init，陈沙克的一篇博文有介绍：http://www.chenshake.com/about-openstack-centos-mirror/ 1$ yum install -y cloud-utils cloud-init parted 修改配置文件/etc/cloud/cloud.cfg ，在cloud_init_modules 下面增加: 1- resolv-conf (5)设置系统能自动获取openstack指定的hostname和ssh-key（可选）编辑/etc/rc.local文件，该文件在开机后会执行，加入以下代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950if [ ! -d /root/.ssh ]; thenmkdir -p /root/.sshchmod 700 /root/.sshfi# Fetch public key using HTTPATTEMPTS=30FAILED=0 while [ ! -f /root/.ssh/authorized_keys ]; docurl -f http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key &gt; /tmp/metadata-key 2&gt;/dev/nullif [ $? -eq 0 ]; thencat /tmp/metadata-key &gt;&gt; /root/.ssh/authorized_keyschmod 0600 /root/.ssh/authorized_keysrestorecon /root/.ssh/authorized_keysrm -f /tmp/metadata-keyecho “Successfully retrieved public key from instance metadata”echo “*****************”echo “AUTHORIZED KEYS”echo “*****************”cat /root/.ssh/authorized_keysecho “*****************”curl -f http://169.254.169.254/latest/meta-data/hostname &gt; /tmp/metadata-hostname 2&gt;/dev/nullif [ $? -eq 0 ]; thenTEMP_HOST=`cat /tmp/metadata-hostname`sed -i “s/^HOSTNAME=.*$/HOSTNAME=$TEMP_HOST/g” /etc/sysconfig/network/bin/hostname $TEMP_HOSTecho “Successfully retrieved hostname from instance metadata”echo “*****************”echo “HOSTNAME CONFIG”echo “*****************”cat /etc/sysconfig/networkecho “*****************”elseecho “Failed to retrieve hostname from instance metadata. This is a soft error so we’ll continue”firm -f /tmp/metadata-hostnameelseFAILED=$(($FAILED + 1))if [ $FAILED -ge $ATTEMPTS ]; thenecho “Failed to retrieve public key from instance metadata after $FAILED attempts, quitting”breakfiecho “Could not retrieve public key from instance metadata (attempt #$FAILED/$ATTEMPTS), retrying in 5 seconds…”sleep 5fidone 或者 1234567891011121314151617181920212223# set a random pass on first bootif [ -f /root/firstrun ]; then dd if=/dev/urandom count=50|md5sum|passwd --stdin root passwd -l root rm /root/firstrunfiif [ ! -d /root/.ssh ]; then mkdir -m 0700 -p /root/.ssh restorecon /root/.sshfi# Get the root ssh key setup# Get the root ssh key setupReTry=0while [ ! -f /root/.ssh/authorized_keys ] &amp;&amp; [ $ReTry -lt 10 ]; do sleep 2 curl -f http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key &gt; /root/.ssh/pubkey if [ 0 -eq 0 ]; then mv /root/.ssh/pubkey /root/.ssh/authorized_keys fi ReTry=$[Retry+1]donechmod 600 /root/.ssh/authorized_keys &amp;&amp; restorecon /root/.ssh/authorized_keys 主要目的就是获取hostname和公钥 (6)其他 route命令查看一下路由表 查看/etc/ssh/sshd_conf中PermitRootLogin是不是为yes 清除操作记录 123456789101112131415161718192021222324252627282930清除登陆系统成功的记录[root@localhost root]# echo &gt; /var/log/wtmp //此文件默认打开时乱码，可查到ip等信息[root@localhost root]# last //此时即查不到用户登录信息清除登陆系统失败的记录[root@localhost root]# echo &gt; /var/log/btmp //此文件默认打开时乱码，可查到登陆失败信息[root@localhost root]# lastb //查不到登陆失败信息 清除历史执行命令[root@localhost root]# history -c //清空历史执行命令[root@localhost root]# echo &gt; ./.bash_history //或清空用户目录下的这个文件即可 导入空历史记录[root@localhost root]# vi /root/history //新建记录文件[root@localhost root]# history -c //清除记录 [root@localhost root]# history -r /root/history.txt //导入记录 [root@localhost root]# history //查询导入结果example [root@localhost root]# vi /root/history[root@localhost root]# history -c [root@localhost root]# history -r /root/history.txt [root@localhost root]# history [root@localhost root]# echo &gt; /var/log/wtmp [root@localhost root]# last[root@localhost root]# echo &gt; /var/log/btmp[root@localhost root]# lastb [root@localhost root]# history -c [root@localhost root]# echo &gt; ./.bash_history[root@localhost root]# history 关闭虚拟机 3、宿主机操作资料：KVM镜像管理利器-guestfish使用详解 1）安装guestfish套件安装 1$ yum install libguestfs-tools 2）压缩镜像文件 1$ virt-sparsify --compress centos7-dis.qcow2 centos7-dis-cloud.qcow2 镜像制作完成 上传openstack 二、参考文档：penStack镜像制作-CentOS openstack镜像制作思路、指导及问题总结 openstack制作centos6.5镜像 制作OpenStack上使用的CentOS系统镜像 KVM镜像管理利器-guestfish使用详解 CentOS清除用户登录记录和命令历史方法]]></content>
      <categories>
        <category>云计算</category>
      </categories>
      <tags>
        <tag>openstack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware共享文件夹设置方法]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F27%2FVMware%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6%E5%A4%B9%E8%AE%BE%E7%BD%AE%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[一、安装包依赖：12$ yum -y install kernel-devel-$(uname -r) $ yum -y install net-tools perl gcc gcc-c++ 二、安装vmtool123456$ mount /dev/cdrom /home/tmp$ cp /home/tmp/VMwareTools-9.6.0-1294478.tar.gz /tmp$ cd /tmp$ tar -zxvf VMwareTools-9.6.0-1294478.tar.gz$ cd vmware-tools-distrib$ ./vmware-install.pl 按提示操作即可。 三、问题有/mnt/hgfs但没有共享文件的解决方法： 12$ mount -t vmhgfs .host:/ /mnt/hgfsError: cannot mount filesystem: No such device 这时不能用mount工具挂载，而是得用vmhgfs-fuse，需要安装工具包 123$ yum install open-vm-tools-devel -y有的源的名字并不一定为open-vm-tools-devel(centos) ，而是open-vm-dkms(unbuntu)执行：vmhgfs-fuse .host:/ /mnt/hgfs 此时进入/mnt/hgfs就能看到你设置的共享文件夹了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>vmware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSL证书制作]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F27%2FSSL%E8%AF%81%E4%B9%A6%E5%88%B6%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一、环境OS:centos 7.3 二、步骤1、安装openssl1$ yum install opensll 2、制作CA证书12$ openssl genrsa -des3 -out my-ca.key 2048$ openssl req -new -x509 -days 3650 -key my-ca.key -out my-ca.crt 3、生成服务器证书123$ openssl genrsa -des3 -out mars-server.key 1024$ openssl req -new -key mars-server.key -out mars-server.csr$ openssl x509 -req -in mars-server.csr -out mars-server.crt -sha1 -CA my-ca.crt -CAkey my-ca.key -CAcreateserial -days 3650 4、生成无密码密钥1$ openssl rsa -in mars-server.key -out mars-server.key.insecure]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>ssl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[proxy_pass反向代理配置中url后面加不加/的说明]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F23%2Fproxy-pass%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE%E4%B8%ADurl%E5%90%8E%E9%9D%A2%E5%8A%A0%E4%B8%8D%E5%8A%A0-%E7%9A%84%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[1 环境OS:centos7 nginx _proxy服务器：192.168.1.23 web服务器：192.168.1.5 2 情况说明2.1 path路径后面加”/”2.1.1 情况一NGINX配置 12345678910111213[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090/; &#125;&#125; 这样，访问http://192.168.1.23/proxy/就会被代理到http://192.168.1.5:8090/。匹配的proxy目录不需要存在根目录/var/www/html里面 注意，终端里如果访问http://192.168.1.23/proxy（即后面不带”/”），则会访问失败！因为proxy_pass配置的url后面加了”/” 访问结果如下 12345678910[root@localhost conf.d]# curl http://192.168.1.23/proxy/this is 192.168.1.5[root@localhost conf.d]# curl http://192.168.1.23/proxy&lt;html&gt;&lt;head&gt;&lt;title&gt;301 Moved Permanently&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor="white"&gt;&lt;center&gt;&lt;h1&gt;301 Moved Permanently&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.10.3&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 页面访问http://103.110.186.23/proxy的时候，会自动加上”/”（同理是由于proxy_pass配置的url后面加了”/”），并反代到http://103.110.186.5:8090的结果。 2.1.2 情况二123456789101112131415161718[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service## 那么访问http://192.168.1.23/proxy或http://192.168.1.23/proxy/，都会失败！## 这样配置后，访问http://192.168.1.23/proxy/就会被反向代理到http://192.168.1.5:8090/proxy/ 2.1.3 情况三1234567891011121314151617[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090/haha/;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service[root@localhost conf.d]# curl http://192.168.1.23/proxy/192.168.1.5 haha-index.html 这样配置的话，访问http://103.110.186.23/proxy代理到http://192.168.1.5:8090/haha/ 2.1.4 情况四相对于第三种配置的url不加”/” 1234567891011121314151617181920212223[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy/ &#123; proxy_pass http://192.168.1.5:8090/haha;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service[root@localhost conf.d]# curl http://192.168.1.23/proxy/index.html192.168.1.5 hahaindex.html##################################### 上面配置后，访问http://192.168.1.23/proxy/index.html就会被代理到http://192.168.1.5:8090/hahaindex.html同理，访问http://192.168.1.23/proxy/test.html就会被代理到http://192.168.1.5:8090/hahatest.html[root@localhost conf.d]# curl http://192.168.1.23/proxy/index.html192.168.1.5 hahaindex.html 注意，这种情况下，不能直接访问http://192.168.1.23/proxy/，后面就算是默认的index.html文件也要跟上，否则访问失败！ ———————————————————————————————————————————上面四种方式都是匹配的path路径后面加”/”，下面说下path路径后面不带”/”的情况： 2.2 path路径后面不加”/”2.2.1 情况一proxy_pass后面url带”/”： 123456789101112131415[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090/;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service 2.2.2 情况二，proxy_pass后面url不带”/” 12345678910111213141516[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service[root@localhost conf.d]# 这样配置的话，访问http://103.110.186.23/proxy会自动加上”/”（即变成http://103.110.186.23/proxy/），代理到192.168.1.5:8090/proxy/ 2.2.3 情况三123456789101112131415[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090/haha/;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service 这样配置的话，访问http://103.110.186.23/proxy会自动加上”/”（即变成http://103.110.186.23/proxy/），代理到http://192.168.1.5:8090/haha/ 2.2.4 情况四相对于第三种配置的url不加”/” 123456789101112131415[root@localhost conf.d]# cat test.confserver &#123;listen 80;server_name localhost;location / &#123;root /var/www/html;index index.html;&#125; location /proxy &#123; proxy_pass http://192.168.1.5:8090/haha;&#125;&#125;[root@localhost conf.d]# service nginx restartRedirecting to /bin/systemctl restart nginx.service 这样配置的话，访问http://103.110.186.23/proxy，和第三种结果一样，同样被代理到http://192.168.1.5:8090/haha/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>nginx</tag>
        <tag>运维工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xmanager远程连接CentOS7]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F23%2FXmanager%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5CentOS7%2F</url>
    <content type="text"><![CDATA[安装epel源1yum install -y epel-release 安装lightdm和xfce12yum install -y lightdm yum groupinstall -y xfce 修改配置文件1vim /etc/lightdm/lightdm.conf 内容如下 123[XDMCPServer]enabled=trueport=177 将Display Manager切换为lightdm1systemctl disable gdm &amp;&amp; systemctl enable lightdm 启动lightdm1systemctl start lightdm 关闭防火墙1systemctl stop firewalld.service 登录打开Xmanger客户端，选择XDMCP并输入服务器的ip，回车运行即可。输入账号密码然后就出现下图：（如果正常跳过这步）或者出现黑屏提示无法建立连接这是因为刚开始安装的是Gnome，所以系统默认使用它，现在要改成Xfce，最简单的方法就是把xfce.desktopz之外的文件都干掉。 1234cd /usr/share/xsessions/mkdir bakmv gnome* baksystemctl restart lightdm 重新连接一切正常操作之后就成功连接了。然后就可以快速便捷的工作了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>运维工具</tag>
        <tag>xmanager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编辑rc.local启动命令执行不成功处理]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F23%2F%E7%BC%96%E8%BE%91rc-local%E5%90%AF%E5%8A%A8%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%E4%B8%8D%E6%88%90%E5%8A%9F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[问题举例：rc.local内容如下123456789101112131415#!/bin/bash# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES## It is highly advisable to create own systemd services or udev rules# to run scripts during boot instead of using this file.## In contrast to previous versions due to parallel execution during boot# this script will NOT be run after all other services.## Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure# that this script will be executed during boot.touch /var/lock/subsys/localmount -t cifs -o username=backup,password=xxxxxxx //192.168.1.170/192.168.1.11/ /mnt/backup_data//usr/local/tomcat/bin/startup.sh 挂载命令执行了，但tomcat没有启动 问题原因及处理方法原因因java使用的是解压缩版，在/etc/profile内添加java的环境变量，系统启动时先执行的是rc.local,因此tomcat启动失败 解决办法方法1在rc.local内添加java的环境变量命令（必须放在tomcat启动命令前） 12export JAVA_HOME=/usr/local/java/jdk1.6.0_18export JRE_HOME=/usr/local/java/jdk1.6.0_18/jre 方法2在tomcat的启动脚本内添加java路径分别在tomcat_home/bin目录内的catalina.sh ，setclasspath.sh脚本前面指定JAVA_HOME路径 12export JAVA_HOME=/usr/local/java/jdk1.6.0_18export JRE_HOME=/usr/local/java/jdk1.6.0_18/jre]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>tomcat</tag>
        <tag>运维开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker私有仓库搭建]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F23%2Fdocker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[搭建仓库安装docker-ce,过程省略…….；运行以下命令启动仓库1$ docker run --name registry --restart always -d -p 5000:5000 -v /.registry:/var/lib/registry registry 修改配置修改所有需要使用私有仓库的docker服务器（包括仓库服务器）的docker配置，在/etc/docker目录创建daemon.json文件内容如下：123456# 下面这句表示表示开启5000端口的非安全模式，也就是http模式，否则在push或pull时会报https错误&#123; "insecure-registries":["192.168.1.11:5000"] &#125;# 下面这句是使用阿里云镜像加速，提高外网官方仓库的下载速度，这里一起列出了，不是必须要添加的，和私有仓库没有关系。&#123; "registry-mirrors": ["https://cz0az3lb.mirror.aliyuncs.com"]&#125; 可以同时设置，写法如下 1234&#123;"insecure-registries":["192.168.1.118:5000"],"registry-mirrors": ["http://192.168.1.118:5001"]&#125; 重启docker服务，配置生效 查询仓库镜像列表12345678910[root@zabbix-11 docker]# curl -XGET http://192.168.1.11:5000/v2/_catalog&#123;"repositories":["nginx"]&#125;# 显示镜像nginx# 查询镜像版本``` elixir[root@zabbix-11 docker]# curl -XGET http://192.168.1.11:5000/v2/nginx/tags/list&#123;"name":"nginx","tags":["1","1.13.7"]&#125;#查询nginx镜像的版本号，有1/1.13.7版本 以上查询也可以在web页面查询，复制命令后方http链接地址就行。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>私有仓库</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zabbix3.4.7监控日志]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F23%2Fzabbix3-4-7%E7%9B%91%E6%8E%A7%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[一、日志item介绍下面介绍zabbix另一个“重量级”的功能——日志文件监控，它最主要的是监控日志文件中有没有某个字符串的表达式，对应日志轮转与否，zabbix都支持。在配置Item的时候，Type选择Zabbix agent (active)，这里主要需要配置的是Key。下面是监控日志的两种key——log和logtr。 log[/path/to/some/file,,,,,] logtr[/path/to/some/filename_format,,,,,] ◆ regexp：要匹配内容的正则表达式，或者直接写你要检索的内容也可以，例如我想检索带ERROR关键词的记录 ◆ encoding：编码相关，留空即可 ◆maxlines：一次性最多提交多少行，这个参数覆盖配置文件zabbxi_agentd.conf中的’MaxLinesPerSecond’，我们也可以留空 ◆ mode：默认是all，也可以是skip，skip会跳过老数据 ◆ output：输出给zabbixserver的数据。可以是\1、\2一直\9，\1表示第一个正则表达式匹配出得内容，\2表示第二个正则表达式匹配错的内容。 如果仔细看可以发现，第一个参数不一样，logrt的第一个参数可以使用正则表达式。针对日志回滚用得，例如我们每天都切割nginx日志，日志名位www.a.com_2015-01-01.log、www.a.com_2015-01-02.log等等，使用log肯定不合适，如果文件名使用正则，那么新增的日志文件会立即加入监控。 备注：不管新日志、老日志，只要他们有变更，zabbix都会监控。 只要配置了，Zabbix会根据的正则表达式来匹配日志中的内容。注意，一定要保证Zabbix用户对日志文件有可读权限，否则这个Item的状态会变成“unsupported”。 二、监控原理及注意事项1、Zabbix Server和Zabbix Agent会追踪日志文件的大小和最后修改时间，并且分别记录在字节计数器和最新的时间计数器中。 2、Agent会从上次读取日志的地方开始读取日志。 3、字节计数器和最新时间计数器的数据会被记录在Zabbix数据库，并且发送给Agent，这样能够保证Agent从上次停止的地方开始读取日志。 4、当日志文件大小小于字节计数器中的数字时，字节计数器会变为0，从头开始读取文件。 5、所有符合配置的文件，都会被监控。 6、一个目录下的多个文件如果修改时间相同，会按照字母顺序来读取。 7、到每个Update interval的时间时，Agent会检查一次目录下的文件。 8、Zabbix Agent每秒发送日志量，有一个日志行数上限，防止网络和CPU负载过高，这个数字在zabbix_agentd.conf中的MaxLinePerSecond。 9、在logtr中，正则表达式只对文件名有效，对文件目录无效。 三、日志监控配置请确保Agent有如下两项配置 1、Hostname设定为Server创建主机是填写的Host name，必须一致 2、ServerActive设定为Server的IP Host&gt;&gt;目标主机&gt;&gt;监控项&gt;&gt;创建监控项，如下： 1、log方式 2、logrt方式 说明： type必须选择zabbix agent（active），因为数据是zabbix被监控的主动提交给server key：log[/var/log/message,error]，我们这里是监控的系统日志，打印出带有error的行，大家也可以去监控其他的日志，mysql、nginx等等都是可以的。 key: logrt[“/mnt/backup/log/[0-9]+_[0-9]+.log”,END],监控备份日志，例如20180305_113523.log,正则表达式[0-9]+_[0-9]+.log，具体可以百度学习正则表达式写法；“”双引号在使用表达式时 候最好用上，否则在保存时可能报语法错误； log time format：MMpddphh:mm:ss，对应日志的行头Sep 14 07:32:38，y表示年、M表示月、d表示日、p和:一个占位符，h表示小时，m表示分钟，s表示秒。 四、结果查看切换到最新数据里面，找到相应数据，如下是我的监控截图 五、触发器设置创建触发器，在周期24h内，写入日志则备份正常，否则告警；这里如下简单设置]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins邮件通知]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F23%2Fjenkins%E9%82%AE%E4%BB%B6%E9%80%9A%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[1 简介jenkins集成mail通知服务不再说明，功能简单，这里简单说下常用mail插件Email Extension Plugin 2 安装Email Extension Plugin 2.1 配置Email系统管理&gt;系统设置&gt;Extended E-mail Notification 下面是我的配置，改动不大，邮件通知帐号在系统集成email处配置，这里主要提下插件通知模版 2.2 配置默认模版Default Content Type配置为html 2.2.1 标题1构建通知：$PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS! 2.2.2 内容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;&lt;title&gt;$&#123;ENV, var="JOB_NAME"&#125;-第$&#123;BUILD_NUMBER&#125;次构建日志&lt;/title&gt;&lt;/head&gt;&lt;body leftmargin="8" marginwidth="0" topmargin="8" marginheight="4" offset="0"&gt; &lt;table width="95%" cellpadding="0" cellspacing="0" style="font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif"&gt; &lt;tr&gt; &lt;td&gt; &lt;h2&gt; &lt;font&gt;来自Mr.Jenkins的邮件通知&lt;/font&gt; &lt;/h2&gt; &lt;font&gt;(本邮件是程序自动下发的，请勿回复！)&lt;/font&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;br /&gt; &lt;b&gt;&lt;font color="#0B610B"&gt;构建信息&lt;/font&gt;&lt;/b&gt; &lt;hr size="2" width="100%" align="center" /&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;ul&gt; &lt;li&gt;项目名称：$&#123;PROJECT_NAME&#125;&lt;/li&gt; &lt;li&gt;构建编号：$&#123;BUILD_NUMBER&#125;&lt;/li&gt; &lt;li&gt;构建状态：$&#123;BUILD_STATUS&#125;&lt;/li&gt; &lt;li&gt;触发原因：$&#123;CAUSE&#125;&lt;/li&gt; &lt;li&gt;构建地址：$&#123;BUILD_URL&#125;&lt;/li&gt; &lt;li&gt;构建日志：&lt;a href="$&#123;BUILD_URL&#125;console"&gt;$&#123;BUILD_URL&#125;console&lt;/a&gt;&lt;/li&gt; &lt;li&gt;单元测试报告：&lt;a href="$&#123;BUILD_URL&#125;testReport/"&gt;$&#123;BUILD_URL&#125;testReport/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;工作目录：&lt;a href="$&#123;PROJECT_URL&#125;ws"&gt;$&#123;PROJECT_URL&#125;ws&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;b&gt;&lt;font color="#0B610B"&gt;构建日志:&lt;/font&gt;&lt;/b&gt; &lt;hr size="2" width="100%" align="center" /&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;textarea cols="80" rows="30" readonly="readonly" style="font-family: Courier New"&gt;$&#123;BUILD_LOG&#125;&lt;/textarea&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/body&gt;&lt;/html&gt; 3 添加项目构建 4 通知展示]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker集群WEB管理工具Shipyard]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F22%2FDocker%E9%9B%86%E7%BE%A4WEB%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7Shipyard%2F</url>
    <content type="text"><![CDATA[一、 说明Shipyard部署总体较为简单，有一键部署脚本，只需执行相应命令就可以实现部署，但在部署中还是出现很多问题，主要是shipyard官方网站无法访问，无法使用官方一键脚本部署，使用网上找到的修改版，里面有部分内容有所缺失，现已修改。 二、环境centos:7.4 master: 192.168.1.44 node1: 192.168.1.12 node2: 192.168.1.18 docker version: docker-ce-18.03.0-ce 三、安装步骤安装之前需要部署好docker环境 1、master节点执行一件部署命令12345678910111213141516171819202122232425# 正常直接从官方拉取脚本执行就行了，命令如下$ curl -s https://shipyard-project.com/deploy | bash -s[root@master ~]# curl -s https://shipyard-project.com/deploy | bash -sDeploying Shipyard -&gt; Starting DatabaseUnable to find image 'rethinkdb:latest' locallyTrying to pull repository xxx.mirror.aliyuncs.com/rethinkdb ...Pulling repository xxx.mirror.aliyuncs.com/rethinkdbTrying to pull repository docker.io/library/rethinkdb ...latest: Pulling from docker.io/library/rethinkdbDigest: sha256:29640c7d5015832c40305ad5dcc5d0996ce79b87f7e32d2fd99c9d65ad9414d4 -&gt; Starting Discovery -&gt; Starting Cert Volume -&gt; Starting Proxy -&gt; Starting Swarm Manager -&gt; Starting Swarm Agent -&gt; Starting ControllerWaiting for Shipyard on 192.168.1.44:8080 Shipyard available at http://192.168.1.44:8080Username: admin Password: shipyard# 或者下载脚本后本地执行，命令如下$ sh deploy 执行脚本的时候会自动下载相关镜像，也可以事先下载镜像文件 123456[root@master ~]# docker pull alpine[root@master ~]# docker pull library/rethinkdb[root@master ~]# docker pull microbox/etcd[root@master ~]# docker pull shipyard/docker-proxy[root@master ~]# docker pull swarm[root@master ~]# docker pull shipyard/shipyard 通过访问http://IP：8080登录shipyard,默认帐号密码：admin shipyard 2、分别在node节点执行如下命令，添加节点到shipyard1234# 部署机就是master节点$ curl -sSL https://shipyard-project.com/deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s# 或者通过本地脚本执行命令$ cat deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s 四、问题总结注意项目1：—————————————————————————————————————上面安装shipyard的脚本是英文版的，其实还有中文版的脚本，下面两种都可以使用：（两个地址都失效） 1）安装shipyard 12$ curl -sSL http://dockerclub.net/public/script/deploy | bash -s ==&gt; 中文版$ curl -sSL https://shipyard-project.com/deploy | bash -s ==&gt; 英文版 2）添加node节点 12$ curl -sSL http://dockerclub.net/public/script/deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s ==&gt; 中文版$ curl -sSL https://shipyard-project.com/deploy | ACTION=node DISCOVERY=etcd://&lt;shipyard部署机ip&gt; bash -s ==&gt; 英文版 3）删除shipyard（在节点机上执行，就会将节点从shipyard管理里踢出） 12$ curl http://dockerclub.net/public/script/deploy | ACTION=remove bash -s ==&gt; 中文版$ curl -sSL https://shipyard-project.com/deploy | ACTION=remove bash -s ==&gt; 英文版 ————————————————————————————————————— 问题项目2：————————————————————————————————————— 1）安装shipyard前不需要部署swarm集群，一键包已包含集群建设 2）部署后无法发现节点，容器页面报错，意思是到节点IP:3375端口无法建立连接，500错误；根据排查，发现所有节点swarm_manager容器都没有开启3375端口，于是在脚本里面修改 容器启动命令，添加开启3375端口，具体看下方脚本； 3）在shipyard web页面，master自身发现较慢，不知道什么原因 4）中文版shipard创建容器的时候，设置端口映射但不生效，使用英文版没有问题，未发现问题原因 5）shipyard只适合较小规模docker集群，功能上已跟不上现阶段的docker集群需求。 ————————————————————————————————————— deploy脚本这个是中文版的脚本，和官方的区别就是修改了shipyard镜像文件，下方标注部分说明 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375#!/bin/bashif [ "$1" != "" ] &amp;&amp; [ "$1" = "-h" ]; then echo "Shipyard Deploy uses the following environment variables:" echo " ACTION: this is the action to use (deploy, upgrade, node, remove)" echo " DISCOVERY: discovery system used by Swarm (only if using 'node' action)" echo " IMAGE: this overrides the default Shipyard image" echo " PREFIX: prefix for container names" echo " SHIPYARD_ARGS: these are passed to the Shipyard controller container as controller args" echo " TLS_CERT_PATH: path to certs to enable TLS for Shipyard" echo " PORT: specify the listen port for the controller (default: 8080)" echo " IP: specify the address at which the controller or node will be available (default: eth0 ip)" echo " PROXY_PORT: port to run docker proxy (default: 2375)" exit 1fiif [ -z "`which docker`" ]; then echo "You must have the Docker CLI installed on your \$PATH" echo " See http://docs.docker.com for details" exit 1fiACTION=$&#123;ACTION:-deploy&#125;#官方IMAGE=$&#123;IMAGE:-shipyard/shipyard:latest&#125;#备注官方shipyard:latest标签镜像似乎有问题，实际使用shipyard:master标签镜像IMAGE=$&#123;IMAGE:-dockerclub/shipyard:latest&#125;PREFIX=$&#123;PREFIX:-shipyard&#125;SHIPYARD_ARGS=$&#123;SHIPYARD_ARGS:-""&#125;TLS_CERT_PATH=$&#123;TLS_CERT_PATH:-&#125;CERT_PATH="/etc/shipyard"PROXY_PORT=$&#123;PROXY_PORT:-2375&#125;SWARM_PORT=3375SHIPYARD_PROTOCOL=httpSHIPYARD_PORT=$&#123;PORT:-8080&#125;SHIPYARD_IP=$&#123;IP&#125;DISCOVERY_BACKEND=etcdDISCOVERY_PORT=4001DISCOVERY_PEER_PORT=7001ENABLE_TLS=0CERT_FINGERPRINT=""LOCAL_CA_CERT=""LOCAL_SSL_CERT=""LOCAL_SSL_KEY=""LOCAL_SSL_CLIENT_CERT=""LOCAL_SSL_CLIENT_KEY=""SSL_CA_CERT=""SSL_CERT=""SSL_KEY=""SSL_CLIENT_CERT=""SSL_CLIENT_KEY=""show_cert_help() &#123; echo "To use TLS in Shipyard, you must have existing certificates." echo "The certs must be named ca.pem, server.pem, server-key.pem, cert.pem and key.pem" echo "If you need to generate certificates, see https://github.com/ehazlett/certm for examples."&#125;check_certs() &#123; if [ -z "$TLS_CERT_PATH" ]; then return fi if [ ! -e $TLS_CERT_PATH ]; then echo "Error: unable to find certificates in $TLS_CERT_PATH" show_cert_help exit 1 fi if [ "$PROXY_PORT" = "2375" ]; then PROXY_PORT=2376 fi SWARM_PORT=3376 SHIPYARD_PROTOCOL=https LOCAL_SSL_CA_CERT="$TLS_CERT_PATH/ca.pem" LOCAL_SSL_CERT="$TLS_CERT_PATH/server.pem" LOCAL_SSL_KEY="$TLS_CERT_PATH/server-key.pem" LOCAL_SSL_CLIENT_CERT="$TLS_CERT_PATH/cert.pem" LOCAL_SSL_CLIENT_KEY="$TLS_CERT_PATH/key.pem" SSL_CA_CERT="$CERT_PATH/ca.pem" SSL_CERT="$CERT_PATH/server.pem" SSL_KEY="$CERT_PATH/server-key.pem" SSL_CLIENT_CERT="$CERT_PATH/cert.pem" SSL_CLIENT_KEY="$CERT_PATH/key.pem" CERT_FINGERPRINT=$(openssl x509 -noout -in $LOCAL_SSL_CERT -fingerprint -sha256 | awk -F= '&#123;print $2;&#125;') if [ ! -e $LOCAL_SSL_CA_CERT ] || [ ! -e $LOCAL_SSL_CERT ] || [ ! -e $LOCAL_SSL_KEY ] || [ ! -e $LOCAL_SSL_CLIENT_CERT ] || [ ! -e $LOCAL_SSL_CLIENT_KEY ]; then echo "Error: unable to find certificates" show_cert_help exit 1 fi ENABLE_TLS=1&#125;# container functionsstart_certs() &#123; ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-certs \ -v $CERT_PATH \ alpine \ sh) if [ $ENABLE_TLS = 1 ]; then docker cp $LOCAL_SSL_CA_CERT $PREFIX-certs:$SSL_CA_CERT docker cp $LOCAL_SSL_CERT $PREFIX-certs:$SSL_CERT docker cp $LOCAL_SSL_KEY $PREFIX-certs:$SSL_KEY docker cp $LOCAL_SSL_CLIENT_CERT $PREFIX-certs:$SSL_CLIENT_CERT docker cp $LOCAL_SSL_CLIENT_KEY $PREFIX-certs:$SSL_CLIENT_KEY fi&#125;remove_certs() &#123; docker rm -fv $PREFIX-certs &gt; /dev/null 2&gt;&amp;1&#125;get_ip() &#123; if [ -z "$SHIPYARD_IP" ]; then SHIPYARD_IP=`docker run --rm --net=host alpine ip route get 8.8.8.8 | awk '&#123; print $7; &#125;'` fi&#125;start_discovery() &#123; get_ip ID=$(docker run \ -ti \ -d \ -p 4001:4001 \ -p 7001:7001 \ --restart=always \ --name $PREFIX-discovery \ microbox/etcd:latest -addr $SHIPYARD_IP:$DISCOVERY_PORT -peer-addr $SHIPYARD_IP:$DISCOVERY_PEER_PORT)&#125;remove_discovery() &#123; docker rm -fv $PREFIX-discovery &gt; /dev/null 2&gt;&amp;1&#125;start_rethinkdb() &#123; ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-rethinkdb \ rethinkdb)&#125;remove_rethinkdb() &#123; docker rm -fv $PREFIX-rethinkdb &gt; /dev/null 2&gt;&amp;1&#125;start_proxy() &#123; TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="-e SSL_CA=$SSL_CA_CERT -e SSL_CERT=$SSL_CERT -e SSL_KEY=$SSL_KEY -e SSL_SKIP_VERIFY=1" fi # Note: we add SSL_SKIP_VERIFY=1 to skip verification of the client # certificate in the proxy image. this will pass it to swarm that # does verify. this helps with performance and avoids certificate issues # when running through the proxy. ultimately if the cert is invalid # swarm will fail to return. ID=$(docker run \ -ti \ -d \ -p $PROXY_PORT:$PROXY_PORT \ --hostname=$HOSTNAME \ --restart=always \ --name $PREFIX-proxy \ -v /var/run/docker.sock:/var/run/docker.sock \ -e PORT=$PROXY_PORT \ --volumes-from=$PREFIX-certs $TLS_OPTS\ shipyard/docker-proxy:latest)&#125;remove_proxy() &#123; docker rm -fv $PREFIX-proxy &gt; /dev/null 2&gt;&amp;1&#125;start_swarm_manager() &#123; get_ip TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="--tlsverify --tlscacert=$SSL_CA_CERT --tlscert=$SSL_CERT --tlskey=$SSL_KEY" fi EXTRA_RUN_OPTS="" if [ -z "$DISCOVERY" ]; then DISCOVERY="$DISCOVERY_BACKEND://discovery:$DISCOVERY_PORT" EXTRA_RUN_OPTS="--link $PREFIX-discovery:discovery" fi ID=$(docker run \ -ti \ -d \#下面3375端口是我出现无法连接节点后自己添加的，问题是启动容器没有开放3375端口 -p 3375:3375 \ --restart=always \ --name $PREFIX-swarm-manager \ --volumes-from=$PREFIX-certs $EXTRA_RUN_OPTS \ swarm:latest \ m --replication --addr $SHIPYARD_IP:$SWARM_PORT --host tcp://0.0.0.0:$SWARM_PORT $TLS_OPTS $DISCOVERY)&#125;remove_swarm_manager() &#123; docker rm -fv $PREFIX-swarm-manager &gt; /dev/null 2&gt;&amp;1&#125;start_swarm_agent() &#123; get_ip if [ -z "$DISCOVERY" ]; then DISCOVERY="$DISCOVERY_BACKEND://discovery:$DISCOVERY_PORT" EXTRA_RUN_OPTS="--link $PREFIX-discovery:discovery" fi ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-swarm-agent $EXTRA_RUN_OPTS \ swarm:latest \ j --addr $SHIPYARD_IP:$PROXY_PORT $DISCOVERY)&#125;remove_swarm_agent() &#123; docker rm -fv $PREFIX-swarm-agent &gt; /dev/null 2&gt;&amp;1&#125;start_controller() &#123; #-v $CERT_PATH:/etc/docker:ro \ TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="--tls-ca-cert $SSL_CA_CERT --tls-cert=$SSL_CERT --tls-key=$SSL_KEY --shipyard-tls-ca-cert=$SSL_CA_CERT --shipyard-tls-cert=$SSL_CERT --shipyard-tls-key=$SSL_KEY" fi ID=$(docker run \ -ti \ -d \ --restart=always \ --name $PREFIX-controller \ --link $PREFIX-rethinkdb:rethinkdb \ --link $PREFIX-swarm-manager:swarm \ -p $SHIPYARD_PORT:$SHIPYARD_PORT \ --volumes-from=$PREFIX-certs \ $IMAGE \ --debug \ server \ --listen :$SHIPYARD_PORT \ -d tcp://swarm:$SWARM_PORT $TLS_OPTS $SHIPYARD_ARGS)&#125;wait_for_available() &#123; set +e IP=$1 PORT=$2 echo Waiting for Shipyard on $IP:$PORT docker pull ehazlett/curl &gt; /dev/null 2&gt;&amp;1 TLS_OPTS="" if [ $ENABLE_TLS = 1 ]; then TLS_OPTS="-k" fi until $(docker run --rm ehazlett/curl --output /dev/null --connect-timeout 1 --silent --head --fail $TLS_OPTS $SHIPYARD_PROTOCOL://$IP:$PORT/ &gt; /dev/null 2&gt;&amp;1); do printf '.' sleep 1 done printf '\n'&#125;remove_controller() &#123; docker rm -fv $PREFIX-controller &gt; /dev/null 2&gt;&amp;1&#125;if [ "$ACTION" = "deploy" ]; then set -e check_certs get_ip echo "Deploying Shipyard" echo " -&gt; Starting Database" start_rethinkdb echo " -&gt; Starting Discovery" start_discovery echo " -&gt; Starting Cert Volume" start_certs echo " -&gt; Starting Proxy" start_proxy echo " -&gt; Starting Swarm Manager" start_swarm_manager echo " -&gt; Starting Swarm Agent" start_swarm_agent echo " -&gt; Starting Controller" start_controller wait_for_available $SHIPYARD_IP $SHIPYARD_PORT echo "Shipyard available at $SHIPYARD_PROTOCOL://$SHIPYARD_IP:$SHIPYARD_PORT" if [ $ENABLE_TLS = 1 ] &amp;&amp; [ ! -z "$CERT_FINGERPRINT" ]; then echo "SSL SHA-256 Fingerprint: $CERT_FINGERPRINT" fi echo "Username: admin Password: shipyard"elif [ "$ACTION" = "node" ]; then set -e if [ -z "$DISCOVERY" ]; then echo "You must set the DISCOVERY environment variable" echo "with the discovery system used with Swarm" exit 1 fi check_certs echo "Adding Node" echo " -&gt; Starting Cert Volume" start_certs echo " -&gt; Starting Proxy" start_proxy echo " -&gt; Starting Swarm Manager" start_swarm_manager $DISCOVERY echo " -&gt; Starting Swarm Agent" start_swarm_agent echo "Node added to Swarm: $SHIPYARD_IP" elif [ "$ACTION" = "upgrade" ]; then set -e check_certs get_ip echo "Upgrading Shipyard" echo " -&gt; Pulling $IMAGE" docker pull $IMAGE echo " -&gt; Upgrading Controller" remove_controller start_controller wait_for_available $SHIPYARD_IP $SHIPYARD_PORT echo "Shipyard controller updated"elif [ "$ACTION" = "remove" ]; then # ignore errors set +e echo "Removing Shipyard" echo " -&gt; Removing Database" remove_rethinkdb echo " -&gt; Removing Discovery" remove_discovery echo " -&gt; Removing Cert Volume" remove_certs echo " -&gt; Removing Proxy" remove_proxy echo " -&gt; Removing Swarm Agent" remove_swarm_agent echo " -&gt; Removing Swarm Manager" remove_swarm_manager echo " -&gt; Removing Controller" remove_controller echo "Done"else echo "Unknown action $ACTION" exit 1fi 参考资料https://www.cnblogs.com/kevingrace/p/6867820.html https://www.fcwys.cc/index.php/archives/145.html]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>shipyard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle-11g-R2安装教程]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F20%2FOracle-11g-R2%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、安装Oracle前准备1.创建运行oracle数据库的系统用户和用户组 1234567891011121314[sonny@localhost ~]$ su root #切换到rootPassword: [root@localhost sonny]# groupadd oinstall #创建用户组oinstall[root@localhost sonny]# groupadd dba #创建用户组dba[root@localhost sonny]# useradd -g oinstall -g dba -m oracle #创建oracle用户，并加入到oinstall和dba用户组[root@localhost sonny]# passwd oracle #设置用户oracle的登陆密码，不设置密码，在CentOS的图形登陆界面没法登陆Changing password for user oracle.New password: # 密码BAD PASSWORD: The password is shorter than 8 charactersRetype new password: # 确认密码passwd: all authentication tokens updated successfully.[root@localhost sonny]# id oracle # 查看新建的oracle用户uid=1001(oracle) gid=1002(dba) groups=1002(dba)[root@localhost sonny]# 2.创建oracle数据库安装目录123456789101112[sonny@localhost ~]$ su rootPassword: [root@localhost sonny]# mkdir -p /data/oracle #oracle数据库安装目录[root@localhost sonny]# mkdir -p /data/oraInventory #oracle数据库配置文件目录[root@localhost sonny]# mkdir -p /data/database #oracle数据库软件包解压目录[root@localhost sonny]# cd /data[root@localhost data]# ls #创建完毕检查一下（强迫症）database oracle oraInventory[root@localhost data]# chown -R oracle:oinstall /data/oracle #设置目录所有者为oinstall用户组的oracle用户[root@localhost data]# chown -R oracle:oinstall /data/oraInventory[root@localhost data]# chown -R oracle:oinstall /data/database[root@localhost data]# 3.修改OS系统标识oracle默认不支持CentOS系统安装，Oracle Database 11g Release 2 的 OS要求参考： https://docs.oracle.com/cd/E11882_01/install.112/e47689/pre_install.htm#LADBI1106 我安装是64位数据库，On Linux x86-64：Red Hat Enterprise Linux 7 （RHEL 7） 另外，CentOS7.0.1511 基于 RHEL7.2 参考：http://www.linuxidc.com/Linux/2015-12/126283.htm 修改文件 /etc/RedHat-release 12345678910[sonny@localhost data]$ su rootPassword: [root@localhost data]# cat /proc/version Linux version 3.10.0-327.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) ) #1 SMP Thu Nov 19 22:10:57 UTC 2015[root@localhost data]# cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) [root@localhost data]# vi /etc/redhat-release[root@localhost data]# cat /etc/redhat-release redhat-7 [root@localhost data]# 4.安装oracle数据库所需要的软件包重复一遍，我安装时Oracle Database 11g Release 2 64位数据库。 Oracle Database Package Requirements for Linux x86-64 如下：（参考：https://docs.oracle.com/cd/E11882_01/install.112/e47689/pre_install.htm#BABCFJFG） 1234操作系统:Oracle Linux 7 and Red Hat Enterprise Linux 7The following packages (or later versions) must be installed:$ yum install binutils compat-libstdc++ compat-libstdc++-33 elfutils-libelf-devel gcc gcc-c++ glibc-devel glibc-headers ksh libaio-devel libstdc++-devel make sysstat unixODBC-devel binutils-* compat-libstdc++* elfutils-libelf* glibc* gcc-* libaio* libgcc* libstdc++* make* sysstat* unixODBC* wget unzip 5.关闭防火墙 CentOS 7.2默认使用的是firewall作为防火墙12$ systemctl disable firewalld$ systemctl stop firewalld 6.关闭selinux（需重启生效）1234567891011121314[root@localhost /]# vi /etc/selinux/config[root@localhost /]# cat /etc/selinux/config# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled #此处修改为disabled# SELINUXTYPE= can take one of three two values:# targeted - Targeted processes are protected,# minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection.SELINUXTYPE=targeted 7.修改内核参数1234567891011121314151617181920212223242526[sonny@localhost /]$ su rootPassword: [root@localhost /]# vi /etc/sysctl.conf [root@localhost /]# cat /etc/sysct.confcat: /etc/sysct.conf: No such file or directory[root@localhost /]# cat /etc/sysctl.conf # System default settings live in /usr/lib/sysctl.d/00-system.conf.# To override those settings, enter new settings here, or in an /etc/sysctl.d/&lt;name&gt;.conf file## For more information, see sysctl.conf(5) and sysctl.d(5).net.ipv4.icmp_echo_ignore_broadcasts = 1net.ipv4.conf.all.rp_filter = 1fs.file-max = 6815744 #设置最大打开文件数fs.aio-max-nr = 1048576kernel.shmall = 2097152 #共享内存的总量，8G内存设置：2097152*4k/1024/1024kernel.shmmax = 2147483648 #最大共享内存的段大小kernel.shmmni = 4096 #整个系统共享内存端的最大数kernel.sem = 250 32000 100 128net.ipv4.ip_local_port_range = 9000 65500 #可使用的IPv4端口范围net.core.rmem_default = 262144net.core.rmem_max= 4194304net.core.wmem_default= 262144net.core.wmem_max= 1048576[root@localhost /]# 1234567891011121314151617181920212223使配置参数生效[root@localhost /]# sysctl -pnet.ipv4.icmp_echo_ignore_broadcasts = 1net.ipv4.conf.all.rp_filter = 1sysctl: setting key "fs.file-max": Invalid argumentfs.file-max = 6815744 #设置最大打开文件数fs.aio-max-nr = 1048576sysctl: setting key "kernel.shmall": Invalid argumentkernel.shmall = 2097152 #共享内存的总量，8G内存设置：2097152*4k/1024/1024sysctl: setting key "kernel.shmmax": Invalid argumentkernel.shmmax = 2147483648 #最大共享内存的段大小sysctl: setting key "kernel.shmmni": Invalid argumentkernel.shmmni = 4096 #整个系统共享内存端的最大数kernel.sem = 250 32000 100 128sysctl: setting key "net.ipv4.ip_local_port_range": Invalid argumentnet.ipv4.ip_local_port_range = 9000 65500 #可使用的IPv4端口范围net.core.rmem_default = 262144net.core.rmem_max = 4194304net.core.wmem_default = 262144net.core.wmem_max = 1048576[root@localhost /]# 8.解压安装包1234567891011[oracle@localhost /]$ cd /usr/local/src #进入/usr/local/src目录[oracle@localhost src]$ lslinux.x64_11gR2_database_1of2.zip linux.x64_11gR2_database_2of2.zip[oracle@localhost src]$ unzip linux.x64_11gR2_database_1of2.zip -d /data/database/ #解压(省略...)[oracle@localhost src]$ unzip linux.x64_11gR2_database_2of2.zip -d /data/database/ #解压(省略...)[oracle@localhost src]$ su rootPassword: [root@localhost src]# chown -R oracle:oinstall /data/database/database/[root@localhost src]# 二、oracle安装1.图形界面登陆oracle用户： 备注：root用户切换到oracle用户无法安装报错，必须使用oracle直接登录，否则报如下错误 ==Exception in thread “main” java.lang.NoClassDefFoundError== 2.启动oralce安装，到/data/database/database/目录下，执行runInstaller 3.去掉勾，懒得填，个人使用环境不需要自动接收Oracle的安全更新。 4.下一步，只安装数据库软件，个人用不要那些玩意~~5.选择单例安装，前面的所有配置均为单例安装。 6.添加语言7.默认安装版本企业版-Enterprise Edition8.确定数据软件的安装路径，自动读取前面Oracle环境变量中配置的值。9.理论上要创建Database Operation（OSOPER）Group:oper ,个人用，懒得建，就使用dba用户组10.安装检查，按照提示信息一个一个解决。11.一个一个检查package，在准备阶段中漏掉的，此处再安装，有些系统报错是因为现有的包的版本比检测要高，最后忽略即可。（点击Check_Again 多检查几次）12.准备完毕，fuck “Finish”开始安装。14.提示安装成功。安装日志懒得看，再说。 三、配置监听listener1.执行netca 报错 12345678910111213141516171819[@localhost ~]$ netcaOracle Net Services Configuration:## An unexpected error has been detected by HotSpot Virtual Machine:## SIGSEGV (0xb) at pc=0x00007f69a69fcb9d, pid=8033, tid=140092892297024## Java VM: Java HotSpot(TM) 64-Bit Server VM (1.5.0_17-b03 mixed mode)# Problematic frame:# C [libclntsh.so.11.1+0x62ab9d] snlinGetAddrInfo+0x1b1## An error report file with more information is saved as hs_err_pid8033.log## If you would like to submit a bug report, please visit:# http://java.sun.com/webapps/bugreport/crash.jsp#/data/oracle/product/11.2.0/db_1/bin/netca: line 178: 8033 Aborted (core dumped) $JRE $JRE_OPTIONS -classpath $CLASSPATH oracle.net.ca.NetCA $*[oracle@localhost ~]$ 错误原因：安装操作系统是默认主机名localhost造成错误 解决办法： 12345678910111213141516[racle]# cat /etc/sysconfig/network# Created by anaconda[root@localhost oracle]# vi /etc/sysconfig/network #增加HOSTNAME[root@localhost oracle]# cat /etc/sysconfig/network# Created by anacondaHOSTNAME=odb-sonny[root@localhost oracle]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6[root@localhost oracle]# vi /etc/hosts #增加HOSTNAME&lt;/strong&gt;[root@localhost oracle]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 odb-sonny::1 localhost localhost.localdomain localhost6 localhost6.localdomain6[root@localhost oracle]# hostname odb-sonny #执行[root@localhost oracle]# 最后注销当前oracle用户，重新登陆即可！！这次发现打开配置界面正常，安装windows下面配置即可。 四、创建Oracle数据实例Orcl执行dbca命令，启动oracle实例安装界面，剩下的与Windows上安装一样，不废话了： 注意：必须先创建监听，并且监听是启动中，否则报错。 问题总结问题一：root用户切换到oracle用户无法安装报错，必须使用oracle直接登录，否则报如下错误 Exception in thread “main” java.lang.NoClassDefFoundError 问题二：Oracle 安装报错 [INS-06101] IP address of localhost could not be determined 解决方法 出现这种错误是因为主机名和/etc/hosts 文件不一致，只需要把主机名和其IP 写入/etc/hosts 文件，就ok了。 问题三：在ORACLE11G R2 安装ORACLE时出现以下错误： [INS-08109] Unexpected error occurred while validating inputs at state ‘getOCMDetails’. 经GOOGLE看到 http://www.linkedin.com/groups/I-try-clone-oracle-grid-77941.S.38808726 说是使用了 LD_LIBRARY_PATH 环境参数，经查看.bash_profile ，有如下设置： LD_LIBRARY_PATH=$ORACLE_HOME/lib:/lib:/usr/lib; export LD_LIBRARY_PATH 注释掉后，问题解决。 问题四：“#Error in invoking target ‘install’ of makefile ‘/data/oracle/product/11.2.0/db_1/ctx/lib/ins_ctx.mk’”解决方法：打开一个新的终端，使用root身份登入， #vi ORACLE_HOME/ctx/lib/ins_ctx.mk找到ctxhx: $(CTXHXOBJ)$(LINK_CTXHX) $(CTXHXOBJ) $(INSO_LINK)修改为(添加红色部分): ctxhx: $(CTXHXOBJ)-static $(LINK_CTXHX) $(CTXHXOBJ) $(INSO_LINK) /usr/lib64/libc.a 问题五：“#Error in invoking target ‘agent nmhs’ of makefile ‘/home/oracle_11/app/oracle/product/11.2.0/db_1/sysman/lib/ins_emagent.mk’”解决方法：打开一个新的终端，使用root身份登入， #vi $ORACLE_HOME/sysman/lib/ins_emagent.mk找到$(MK_EMAGENT_NMECTL)修改为(添加红色部分)：$(MK_EMAGENT_NMECTL) ==-lnnz11==完成后在错误提示框上retry既可]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[npm私库部署-cnpmjs]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F16%2Fnpm%E7%A7%81%E5%BA%93%E9%83%A8%E7%BD%B2-cnpmjs%2F</url>
    <content type="text"><![CDATA[一、部署cnpm1、获取代码1git clone git://github.com/fengmk2/cnpmjs.org.git 2、创建mysql库Default 123create database cnpmjs;use cnpmjs;source docs/db.sql【db.sql位于cnpmjs.org/docs/db.sql】 3、安装依赖安装依赖其实就是一个 npm install，不过 CNPM 把该指令已经写到 Makefile 里面了，所以直接执行下面的命令就好了。12$ cd cnpmjs.org $ npm install 当然万一你是 Windows 用户或者不会 make，那么还是要用 npm install。1$ npm install --build-from-source --registry=https://registry.npm.taobao.org --disturl=https://npm.taobao.org/mirrors/node 4、修改配置文件修改配置 1vim /cnpmjs.org/config/index.js cnpm提供两个端口：7001和7002，其中7001用于NPM的注册服务，7002用于Web访问。 bindingHost为安装cnpm的服务器ip地址，也就是在浏览器中只能通过访问http://192.168.48.57来访问cnpm以及获取npm的注册服务。 按照之前创建的数据库来进行配置 这里将会列举一些常用的配置项，其余的一些配置项请自行参考 config/index.js 文件。 配置字段参考https://segmentfault.com/a/1190000005946580 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748enableCluster：是否启用 cluster-worker 模式启动服务，默认 false，生产环节推荐为 true;registryPort：API 专用的 registry 服务端口，默认 7001；webPort：Web 服务端口，默认 7002；bindingHost：监听绑定的 Host，默认为 127.0.0.1，如果外面架了一层本地的 Nginx 反向代理或者 Apache 反向代理的话推荐不用改；sessionSecret：session 用的盐；logdir：日志目录；uploadDir：临时上传文件目录；viewCache：视图模板缓存是否开启，默认为 false；enableCompress：是否开启 gzip 压缩，默认为 false；admins：管理员们，这是一个 JSON Object，对应各键名为各管理员的用户名，键值为其邮箱，默认为 &#123; fengmk2: 'fengmk2@gmail.com', admin: 'admin@cnpmjs.org', dead_horse: 'dead_horse@qq.com' &#125;；logoURL：Logo 地址，不过对于我这个已经把 CNPM 前端改得面目全非的人来说已经忽略了这个配置了；adBanner：广告 Banner 的地址；customReadmeFile：实际上我们看到的 cnpmjs.org 首页中间一大堆冗长的介绍是一个 Markdown 文件转化而成的，你可以设置该项来自行替换这个文件；customFooter：自定义页脚模板；npmClientName：默认为 cnpm，如果你有自己开发或者 fork 的 npm 客户端的话请改成自己的 CLI 命令，这个应该会在一些页面的说明处替换成你所写的；backupFilePrefix：备份目录；database：数据库相关配置，为一个对象，默认如果不配置将会是一个 ~/.cnpmjs.org/data.sqlite 的 SQLite；db：数据的库名；username：数据库用户名；password：数据库密码；dialect：数据库适配器，可选 "mysql"、"sqlite"、"postgres"、"mariadb"，默认为 "sqlite"；hsot：数据库地址；port：数据库端口；pool：数据库连接池相关配置，为一个对象；maxConnections：最大连接数，默认为 10；minConnections：最小连接数，默认为 0；maxIdleTime：单条链接最大空闲时间，默认为 30000 毫秒；storege：仅对 SQLite 配置有效，数据库地址，默认为 ~/.cnpmjs/data.sqlite；nfs：包文件系统处理对象，为一个 Node.js 对象，默认是 [fs-cnpm]() 这个包，并且配置在 ~/.cnpmjs/nfs 目录下，也就是说默认所有同步的包都会被放在这个目录下；开发者可以使用别的一些文件系统插件（如上传到又拍云等）,又或者自己去按接口开发一个逻辑层，这些都是后话了；registryHost：暂时还未试过，我猜是用于 Web 页面显示用的，默认为 r.cnpmjs.org；enablePrivate：是否开启私有模式，默认为 false；如果是私有模式则只有管理员能发布包，其它人只能从源站同步包；如果是非私有模式则所有登录用户都能发布包；scopes：非管理员发布包的时候只能用以 scopes 里面列举的命名空间为前缀来发布，如果没设置则无法发布，也就是说这是一个必填项，默认为 [ '@cnpm', '@cnpmtest', '@cnpm-test' ]，据苏千大大解释是为了便于管理以及让公司的员工自觉按需发布；更多关于 NPM scope 的说明请参见 npm-scope；privatePackages：就如该配置项的注释所述，出于历史包袱的原因，有些已经存在的私有包（可能之前是用 Git 的方式安装的）并没有以命名空间的形式来命名，而这种包本来是无法上传到 CNPM 的，这个配置项数组就是用来加这些例外白名单的，默认为一个空数组；sourceNpmRegistry：更新源 NPM 的 registry 地址，默认为 https://registry.npm.taobao.org；sourceNpmRegistryIsCNpm：源 registry 是否为 CNPM，默认为 true，如果你使用的源是官方 NPM 源，请将其设为 false；syncByInstall：如果安装包的时候发现包不存在，则尝试从更新源同步，默认为 true；syncModel：更新模式（不过我觉得是个 typo），有下面几种模式可以选择，默认为 "none";"none"：永不同步，只管理私有用户上传的包，其它源包会直接从源站获取；"exist"：定时同步已经存在于数据库的包；"all"：定时同步所有源站的包；syncInterval：同步间隔，默认为 "10m" 即十分钟；syncDevDependencies：是否同步每个包里面的 devDependencies 包们，默认为 false；badgeSubject：包的 badge 显示的名字，默认为 cnpm；userService：用户验证接口，默认为 null，即无用户相关功能也就是无法有用户去上传包，该部分需要自己实现接口功能并配置，如与公司的 Gitlab 相对接，这也是后话了；alwaysAuth：是否始终需要用户验证，即便是 $ cnpm install 等命令；httpProxy：代理地址设置，用于你在墙内源站在墙外的情况。 下面是index.js配置实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275'use strict';var mkdirp = require('mkdirp');var copy = require('copy-to');var path = require('path');var fs = require('fs');var os = require('os');var version = require('../package.json').version;var root = path.dirname(__dirname);var dataDir = path.join(process.env.HOME || root, '.cnpmjs.org');var config = &#123; version: version, dataDir: dataDir, /** * Cluster mode */ enableCluster: false, numCPUs: os.cpus().length, /* * server configure */ //cnpm提供两个端口：7001和7002，其中7001用于NPM的注册服务，7002用于Web访问。 registryPort: 7001, webPort: 7002, //bindingHost为安装cnpm的服务器ip地址，也就是在浏览器中只能通过访问http://192.168.48.57来访问cnpm以及获取npm的注册服务。 bindingHost: '192.168.1.12', // only binding on 127.0.0.1 for local access // debug mode // if in debug mode, some middleware like limit wont load // logger module will print to stdout debug: process.env.NODE_ENV === 'development', // page mode, enable on development env pagemock: process.env.NODE_ENV === 'development', // session secret sessionSecret: 'cnpmjs.org test session secret', // max request json body size jsonLimit: '10mb', // log dir name logdir: path.join(dataDir, 'logs'), // update file template dir uploadDir: path.join(dataDir, 'downloads'), // web page viewCache viewCache: false, // config for koa-limit middleware // for limit download rates limit: &#123; enable: false, token: 'koa-limit:download', limit: 1000, interval: 1000 * 60 * 60 * 24, whiteList: [], blackList: [], message: 'request frequency limited, any question, please contact fengmk2@gmail.com', &#125;, enableCompress: false, // enable gzip response or not // default system admins admins: &#123; // name: email fengmk2: 'fengmk2@gmail.com', admin: 'admin@cnpmjs.org', dead_horse: 'dead_horse@qq.com', &#125;, // email notification for errors // check https://github.com/andris9/Nodemailer for more informations mail: &#123; enable: false, appname: 'cnpmjs.org', from: 'cnpmjs.org mail sender &lt;adderss@gmail.com&gt;', service: 'gmail', auth: &#123; user: 'address@gmail.com', pass: 'your password' &#125; &#125;, logoURL: 'https://os.alipayobjects.com/rmsportal/oygxuIUkkrRccUz.jpg', // cnpm logo image url adBanner: '', customReadmeFile: '', // you can use your custom readme file instead the cnpm one customFooter: '', // you can add copyright and site total script html here npmClientName: 'cnpm', // use `$&#123;name&#125; install package` packagePageContributorSearch: true, // package page contributor link to search, default is true // max handle number of package.json `dependencies` property maxDependencies: 200, // backup filepath prefix backupFilePrefix: '/cnpm/backup/', /** * database config */ //配置数据库连接信息 database: &#123; db: 'cnpm', username: 'root', password: '123456', // the sql dialect of the database // - currently supported: 'mysql', 'sqlite', 'postgres', 'mariadb' dialect: 'mysql', // custom host; default: 127.0.0.1 host: '192.168.1.12', // custom port; default: 3306 port: 3306, // use pooling in order to reduce db connection overload and to increase speed // currently only for mysql and postgresql (since v1.5.0) pool: &#123; maxConnections: 10, minConnections: 0, maxIdleTime: 30000 &#125;, // the storage engine for 'sqlite' // default store into ~/.cnpmjs.org/data.sqlite storage: path.join(dataDir, 'data.sqlite'), logging: !!process.env.SQL_DEBUG, &#125;, // package tarball store in local filesystem by default nfs: require('fs-cnpm')(&#123; dir: path.join(dataDir, 'nfs') &#125;), // if set true, will 302 redirect to `nfs.url(dist.key)` downloadRedirectToNFS: false, // registry url name registryHost: 'r.cnpmjs.org', /** * registry mode config */ // enable private mode or not // private mode: only admins can publish, other users just can sync package from source npm // public mode: all users can publish enablePrivate: false, // registry scopes, if don't set, means do not support scopes scopes: [ '@cnpm', '@cnpmtest', '@cnpm-test' ], // some registry already have some private packages in global scope // but we want to treat them as scoped private packages, // so you can use this white list. privatePackages: [], /** * sync configs */ // the official npm registry // cnpm wont directly sync from this one // but sometimes will request it for some package infomations // please don't change it if not necessary officialNpmRegistry: 'https://registry.npmjs.com', officialNpmReplicate: 'https://replicate.npmjs.com', // sync source, upstream registry // If you want to directly sync from official npm's registry // please drop them an email first sourceNpmRegistry: 'https://registry.npm.taobao.org', // upstream registry is base on cnpm/cnpmjs.org or not // if your upstream is official npm registry, please turn it off sourceNpmRegistryIsCNpm: true, // if install return 404, try to sync from source registry syncByInstall: true, // sync mode select // none: do not sync any module, proxy all public modules from sourceNpmRegistry // exist: only sync exist modules // all: sync all modules //配置从仓库同步到本地（重要） syncModel: 'exist', // 'none', 'all', 'exist' syncConcurrency: 1, // sync interval, default is 10 minutes syncInterval: '10m', // sync polular modules, default to false // because cnpm can't auto sync tag change for now // so we want to sync popular modules to ensure their tags syncPopular: false, syncPopularInterval: '1h', // top 100 topPopular: 100, // sync devDependencies or not, default is false syncDevDependencies: false, // try to remove all deleted versions from original registry syncDeletedVersions: true, // changes streaming sync syncChangesStream: false, handleSyncRegistry: 'http://127.0.0.1:7001', // badge subject on http://shields.io/ badgePrefixURL: 'https://img.shields.io/badge', badgeSubject: 'cnpm', // custom user service, @see https://github.com/cnpm/cnpmjs.org/wiki/Use-Your-Own-User-Authorization // when you not intend to ingegrate with your company's user system, then use null, it would // use the default cnpm user system userService: null, // always-auth https://docs.npmjs.com/misc/config#always-auth // Force npm to always require authentication when accessing the registry, even for GET requests. alwaysAuth: false, // if you're behind firewall, need to request through http proxy, please set this // e.g.: `httpProxy: 'http://proxy.mycompany.com:8080'` httpProxy: null, // snyk.io root url snykUrl: 'https://snyk.io', // https://github.com/cnpm/cnpmjs.org/issues/1149 // if enable this option, must create module_abbreviated and package_readme table in database //enableAbbreviatedMetadata: false, //配置enableAbbreviatedMetadata为true（默认false）,解决不能同步。重要！ enableAbbreviatedMetadata: true, // global hook function: function* (envelope) &#123;&#125; // envelope format please see https://github.com/npm/registry/blob/master/docs/hooks/hooks-payload.md#payload globalHook: null, opensearch: &#123; host: '', &#125;,&#125;;if (process.env.NODE_ENV === 'test') &#123; config.enableAbbreviatedMetadata = true;&#125;if (process.env.NODE_ENV !== 'test') &#123; var customConfig; if (process.env.NODE_ENV === 'development') &#123; customConfig = path.join(root, 'config', 'config.js'); &#125; else &#123; // 1. try to load `$dataDir/config.json` first, not exists then goto 2. // 2. load config/config.js, everything in config.js will cover the same key in index.js customConfig = path.join(dataDir, 'config.json'); if (!fs.existsSync(customConfig)) &#123; customConfig = path.join(root, 'config', 'config.js'); &#125; &#125; if (fs.existsSync(customConfig)) &#123; copy(require(customConfig)).override(config); &#125;&#125;mkdirp.sync(config.logdir);mkdirp.sync(config.uploadDir);module.exports = config;config.loadConfig = function (customConfig) &#123; if (!customConfig) &#123; return; &#125; copy(customConfig).override(config);&#125;; 5、启动服务搞好配置之后就可以直接启动服务了。1$ node dispatch.js 官方脚本启动官方的其它一些指令，比如你可以用 NPM 的 script 来运行。 1$ npm run start 在 CNPM 里面，npm script 还有下面几种指令 12345npm run dev：调试模式启动；npm run test：跑测试；npm run start：启动 CNPM；npm run status：查看 CNPM 启动状态；npm run stop：停止 CNPM。 6、访问cnpmhttp://192.168.1.12:7002/ 二、问题总结问题1：安装cnmp依赖问题npm 默认使用官方源（国外），速度慢,已报错；可以安装cnpm，使用cnpm install安装（淘宝源） Default 1234//安装cnpm $ npm install -g cnpm --registry=https://registry.npm.taobao.org//使用cnpm执行安装依赖命令 $ cnpm install 问题2：设置同步后报错设置同步后，syncModel设置为exist或all,通过仓库下载模块报如下错误 1[c#0] [error] [connect] sync error: TypeError: Cannot read property ‘findAll’ of null 解决办法： 在index.js文件内设置enableAbbreviatedMetadata: true，问题解决。 参考：https://github.com/cnpm/cnpmjs.org/issues/1236]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
        <tag>部署</tag>
        <tag>npm</tag>
        <tag>cnpmjs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos同步yum源到本地]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F16%2Fcentos%E5%90%8C%E6%AD%A5yum%E6%BA%90%E5%88%B0%E6%9C%AC%E5%9C%B0%2F</url>
    <content type="text"><![CDATA[一、环境os:centos 7.3 1611 应用：yum-utils 互联网源：阿里云 二、步骤删除/etc/yum.repos.d下所有源文件 1、下载源repo到本地1$ wget -O /etc/yum.repos.d/aliyun.repo https://mirrors.aliyun.com/repo/Centos-7.repo 2、安装yum-utils提供reporsync服务1$ yum install yum-utils -y 3、查看yum源仓库标识1234567891011[root@localhost yum.repos.d]# yum repolist已加载插件：fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.com源标识 源名称 状态base/7/x86_64 CentOS-7 - Base - mirrors.aliyun.com 9,591extras/7/x86_64 CentOS-7 - Extras - mirrors.aliyun.com 196updates/7/x86_64 CentOS-7 - Updates - mirrors.aliyun.com 657repolist: 10,444 4、根据源标识同步源到本地目录1[root@localhost ~]# reposync -r base -p /var/www/html/ #这里同步base目录到本地 注意： 部分互联网yum源不支持同步 参考资料http://www.cnblogs.com/chengd/articles/6912938.html https://www.2cto.com/net/201512/455901.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>私有仓库</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7安装独立显卡驱动]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F16%2FCentos7%E5%AE%89%E8%A3%85%E7%8B%AC%E7%AB%8B%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[Centos7 安装独立显卡驱动参考：https://blog.csdn.net/u013378306/article/details/69229919 安装基础依赖环境1$ Yum install gcc kernel-delve -y 注意事项，保证内核版本和源码版本一样，否则，安装报错误6： 查看内核版本：1$ ls /boot | grep vmlinu 查看源码包版本 1rpm -aq | grep kernel-devel 从上面的输出中可以看出内核版本号和内核源码版本。为了解决这个错误，需要从FC官方网站上下载与内核版本对应的源码包进行安装。可以在以下网站下载并安装：http://rpmfind.net/linux/rpm2html/search.php?query=kernel-devel 源码安装1 在英伟达官网下载相应驱动搜索出相应的驱动后，不要直接点，而是右健，Save Link as… 否则，会出现下载半天没动静的情况。 存放的路径上最好不要有中文。 我存放的路径是 ~/Downloads/NVIDIA-Linux-x86_64-346.47.run 2 屏蔽默认带有的nouveau使用su命令切换到root用户下: su root 打开/lib/modprobe.d/dist-blacklist.conf 12345# 将nvidiafb注释掉。# blacklist nvidiafb 然后添加以下语句：blacklist nouveauoptions nouveau modeset=0 3 重建initramfs image步骤1234567891011121314$ mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak$ dracut /boot/initramfs-$(uname -r).img $(uname -r)``` ### 4 修改运行级别为文本模式``` bash$ systemctl set-default multi-user.target``` ### 5 重新启动, 使用root用户登陆``` bash$ reboot 6 查看nouveau是否已经禁用1ls mod | grep nouveau 如果没有显示相关的内容，说明已禁用。 7 进入下载的驱动所在目录123$ chmod +x NVIDIA-Linux-x86_64-346.47.run$ ./NVIDIA-Linux-x86_64-346.47.run 安装过程中，选择accept 如果提示要修改xorg.conf，选择yes 8 修改运行级别回图形模式1systemctl set-default graphical.target 9 重新启动，OK在Applications–Other可以看见NVIDIA X Server Settings菜单。 问题：错误1： 1ERROR: The Nouveau kernel driver is currently in use by your system. This driver is incompatible with the NVIDIA driver, and must be disabled before proceeding. Please consult the NVIDIA driver README and your Linux distribution's documentation for details on how to correctly disable the Nouveau kernel driver. 解释：如果没有执行屏蔽nouveau操作，报以上错误。 错误2： 1unable to find the development too 'cc' in you path; please make sure that you have the package 'gcc 解决办法： 1$ yum install gcc 错误3： 123ERROR: Unable to find the kernel source tree for the currently running kernel. Please make sure you have installed the kernel source files for your kernel and that they are properly configured; on Red Hat Linux systems, for example, be sure you have the 'kernel-source' or 'kernel-devel' RPM installed. If you know the correct kernel source files are installed, you may specify the kernel source path with the '--kernel-source-path' command line option. 解决办法：1$ yum install kernel-delve 错误5： 1ERROR: Unable to find the kernel source tree for the currently running kernel. Please make sure you have installed the kernel source files for your kernel and that they are properly configured; on Red Hat Linux systems, for example, be sure you have the 'kernel-source' or 'kernel-devel' RPM installed. If you know the correct kernel source files are installed, you may specify the kernel source path with the '--kernel-source-path' command line option. 解决方法： 1$ ./NVIDIA-Linux-x86_64-390.67.run --kernel-source-path=/usr/src/kernels/3.10.0-862.3.2.el7.x86_64/ 错误6： 1ERROR: Unable to load the kernel module 'nvidia.ko'. This happens most frequently when this kernel module was built against the wrong or improperly configured kernel sources, with a version of gcc that differs from the one used to build the target kernel, or if another driver, such as nouveau, is present and prevents the NVIDIA kernel module from obtaining ownership of the NVIDIA GPU(s), or no NVIDIA GPU installed in this system is supported by this NVIDIA Linux graphics driver release. Please see the log entries &apos;Kernel module load error&apos; and &apos;Kernel messages&apos; at the end of the file &apos;/var/log/nvidia-installer.log&apos; for more information. 解决办法： 可以通过以下方式查看内核版本和源码包版本：ls /boot | grep vmlinuz如果上面的命令输出中有多个内核，则按grub.conf中指定的文件为准。rpm -aq | grep kernel-develkernel-devel-2.6.35.13-92.fc14.i686从上面的输出中可以看出内核版本号和内核源码版本。为了解决这个错误，需要从FC官方网站上下载与内核版本对应的源码包进行安装。 可以在以下网站下载并安装： http://rpmfind.net/linux/rpm2html/search.php?query=kernel-devel 备注：执行更新内核操作好需要重新执行屏蔽nouveau，及重建initramfs image步骤。 警告： 123456**WARNING: nvidia-installer was forced to guess the X library path '/usr/lib64' and X module path '/usr/lib64/xorg/modules'; these paths were not queryable from the system. If X fails to find the NVIDIA X driver module, please install the `pkg-config` utility and the X.Org SDK/development package for your distribution and reinstall the driver. 字符模式安装警告信息，可忽略。 安装cuda参考：https://blog.csdn.net/claroja/article/details/81034147 错误： 123Installing the CUDA Toolkit in /usr/local/cuda-6.5 ...Missing recommended library: libGLU.soMissing recommended library: libXmu.so 解决：安装第三方软件 123$ yum install freeglut-devel libX11-devel libXi-devel libXmu-devel \$ make mesa-libGLU-devel 测试CUDA1234567[root@fengyun6 ~]# find / -name deviceQuery/root/NVIDIA_CUDA-9.0_Samples/1_Utilities/deviceQuery/usr/local/cuda-9.0/extras/demo_suite/deviceQuery/usr/local/cuda-9.0/samples/1_Utilities/deviceQuery 若出现以下信息，则表示安装成功 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283[root@fengyun6 ~]# /usr/local/cuda-9.0/extras/demo_suite/deviceQuery/usr/local/cuda-9.0/extras/demo_suite/deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking) Detected 1 CUDA Capable device(s) Device 0: "GeForce GTX 1080 Ti" CUDA Driver Version / Runtime Version 9.1 / 9.0 CUDA Capability Major/Minor version number: 6.1 Total amount of global memory: 11178 MBytes (11721113600 bytes) (28) Multiprocessors, (128) CUDA Cores/MP: 3584 CUDA Cores GPU Max Clock rate: 1645 MHz (1.64 GHz) Memory Clock rate: 5505 Mhz Memory Bus Width: 352-bit L2 Cache Size: 2883584 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 2 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Disabled Device supports Unified Addressing (UVA): Yes Device PCI Domain ID / Bus ID / location ID: 0 / 1 / 0 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt; deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.1, CUDA Runtime Version = 9.0, NumDevs = 1, Device0 = GeForce GTX 1080 TiResult = PASS 安装cudnn参考：https://www.cnblogs.com/mar-q/p/7482720.html 下载：https://developer.nvidia.com/rdp/cudnn-archive 安装cudnn1$ tar -xvf cudnn-8.0-linux-x64-v6.0.tgz -C /usr/local/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>驱动</tag>
        <tag>显卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins各种触发方式介绍]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F16%2Fjenkins%E5%90%84%E7%A7%8D%E8%A7%A6%E5%8F%91%E6%96%B9%E5%BC%8F%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[触发远程构建使用svn存储库hooks的post-commit,调用jenkins的api触发job。（存储库更新即触发构建，不能针对某个分支目录更新触发）Build after other projects are built某个projects触发构建后执行构建 Build periodicallyBuild periodically：周期进行项目构建（它不care源码是否发生变化），我的配置如下： 0 2 * （每天2:00 必须build一次源码） Poll SCMPoll SCM：定时检查源码变更（根据SCM软件的版本号），如果有更新就checkout最新code下来，然后执行构建动作。我的配置如下： /5 * （每5分钟检查一次源码变化）可针对某个分支目录更新触发构建]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx反向代理报504超时错误]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F16%2FNginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E6%8A%A5504%E8%B6%85%E6%97%B6%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[一、nginx+tomcat后端为tomcat,nginx代理报504超时错误。问题描述： #错误 121.198.17.123 - - [06/Jul/2018:01:48:57 +0000] "POST /mapbj3/getticket HTTP/1.1" 504 537 "https://XXXXXXXXXX.com/walkcode3/index.html?openId=oB6UW0cF3Z_dnYXnz4tG4OFt7Rt0" "Mozilla/5.0 (Linux; Android 8.1; PACM00 Build/O11019; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.143 Crosswalk/24.53.595.0 XWEB/155 MMWEBSDK/19 Mobile Safari/537.36 MicroMessenger/6.6.6.1300(0x26060638) NetType/WIFI Language/zh_CN MicroMessenger/6.6.6.1300(0x26060638) NetType/WIFI Language/zh_CN miniProgram" "-"2018/07/06 01:48:57 [error] 6#6: *2573 upstream timed out (110: Connection timed out) while connecting to upstream, client: 1.198.17.123, server: , request: "POST /mapbj3/getticket HTTP/1.1", upstream: "http://123.149.236.180:8022/mapbj3//getticket", host: "XXXXXXXX.com", referrer: "https://XXXXXXX.com/walkcode3/index.html?openId=oB6UW0cF3Z_dnYXnz4tG4OFt7Rt0" 1、项目本地访问没问题，通过nginx访问报504错误； 2、重启nginx后正常，反复发生，其它项目代理没有问题； 3、搜索了一大推”NGINX 504 Gateway Time-out tomcat”,都是与php有关的,而默认优化的就是php配置的; 问题处理：修改/etc/nginx/nginx.conf,添加如下信息 1234567891011121314151617181920212223242526272829303132333435363738394041# cat /etc/nginx/nginx.confuser nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; #用于tomcat反向代理,解决nginx 504错误 proxy_connect_timeout 300; proxy_send_timeout 300; proxy_read_timeout 300; proxy_buffer_size 16k; proxy_buffers 4 64k; proxy_busy_buffers_size 128k; proxy_temp_file_write_size 128k; # ps:以timeout结尾配置项时间要配置大点&#125; 二、nginx+php(未验证)问题如上，问题处理添加如下内容 1234567891011121314151617181920212223242526272829303132333435363738394041user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; #用于php反向代理,解决nginx 504错误 #以fastcgi_*配置项是php用的 fastcgi_connect_timeout 1000; fastcgi_send_timeout 1000; fastcgi_read_timeout 1000; fastcgi_buffer_size 64k; fastcgi_buffers 8 128k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; fastcgi_intercept_errors on;&#125; 参考文档https://blog.csdn.net/lcj_star/article/details/76672748https://www.iyunv.com/thread-319236-1-1.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yapi部署]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F16%2Fyapi%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[一、在线安装1、安装nodejs下载压缩包，设置环境变量，这里不祥述。 2、安装mongodb12345678910111213141516171819202122232425262728293031323334353637383940# 添加yum源$ vim /etc/yum.repos.d/mongodb-3.4.repo #添加下面的内容，wq保存。 [mongodb-org-3.4]name=MongoDB Repositorybaseurl=https://repo.mongodb.org/yum/RedHat/$releasever/mongodb-org/3.4/x86_64/gpgcheck= 0enabled=1# 安装mongodbyum install -y mongodb-org# 配置mongod#启动:$ service mongod start[root@CENTSVR247 vendors]# mongo&amp;gt; use admin #切换到admin数据库switched to db admin#创建dba用户&amp;gt; db.createUser(... ... &#123;... ... user: "dba",... ... pwd: "dba",... ... roles: [ &#123; role: "userAdminAnyDatabase", db: "admin" &#125; ]... ... &#125;... ... )# 创建yapi数据库use yapiswitched to db yapi给yapi数据库添加test1用户,权限为读写db.createUser(... ... &#123;... ... user: "test1",... ... pwd: "test1",... ... roles: [... ... &#123; role: "readWrite", db: "yapi" &#125; ... ... ]... ... &#125;... ... ) 3、安装Yapi官方说明：https://yapi.ymfe.org/devops/index.html 方式一：可视化部署12$ npm install -g yapi-cli --registry https://registry.npm.taobao.org$ yapi server 根据提示，浏览器访问 http://部署YApi服务器的IP:9090。 填写完信息后，点击“开始部署”。（大概等待1分钟）退出当前状态 CTRL + C 修改配置 这里我们不急着根据提示进行启动，有些参数我们可以通过修改配置达到。 123456789101112131415161718192021222324#修改config.json$ vim /root/my-yapi/config.json 修改下面的内容（邮箱可以不用163的），wq保存。&#123; "port": "80", "adminAccount": "yizitadmin@yizit.cn", "db": &#123; "servername": "127.0.0.1", "DATABASE": "yapi", "port": "27017" &#125;, "mail": &#123; "enable": true, "host": "smtp.163.com", "port": 465, "from": "可用于发送邮件的163邮箱", "auth": &#123; "user": "163邮箱", "pass": "163邮箱对应的密码或授权码" &#125; &#125;&#125; 启动 切换到部署目录下 cd /root/my-yapi启动服务 1$ node vendors/server/app.js 由于修改了配置，所以直接访问 http://部署YApi服务器的IP/login。 访问http://部署YApi服务器的IP:3000/login 默认用户密码：admin@admin.com ymfe.org 方式二：命令行部署安装yapi 123456$ mkdir yapi$ cd yapi$ git clone https://github.com/YMFE/yapi.git vendors //或者下载 zip 包解压到 vendors 目录$ cp vendors/config_example.json ./config.json //复制完成后请修改相关配置$ cd vendors$ npm install --production --registry https://registry.npm.taobao.org 安装pm2 12$ cd vendors$ npm install -S pm2 初始化及启动yapi 12$ npm run install-server //安装程序会初始化数据库索引和管理员账号，管理员账号名可在 config.json 配置$ node server/app.js //启动服务器后，请访问 127.0.0.1:&#123;config.json配置的端口&#125;，初次运行会有个编译的过程，请耐心等候 使用pm2启动方式 1234# 启动$ npx pm2 start ./server/app.js# 停止$ npx pm2 stop all 二、离线安装 离线安装只能采用命令行部署方式 node安装不再详述。 内网安装mongodb解压mongodb-linux-x86_64-3.6.4.tgz并放入mongodb文件夹中 12$ tar -zxvf mongodb-linux-x86_64-3.6.4.tgz$ mv mongodb-linux-x86_64-3.6.4 mongodb 把mongodb放入环境变量中, 修改~/.bashrc, 加入以下内容 1export PATH=&amp;lt;mongodb文件夹的路径&amp;gt;/bin:$PATH 验证安装 12$ source ~/.bashrc$ mongo --version 创建dbdata/db文件夹和dblog文件夹(请自行确保这些文件夹的读写权限) 12$ mkdir -p dbdata/db$ mkdir dblog 启动mongodb服务 1$ sudo ./mongodb/bin/mongod --fork --dbpath ./dbdata --logpath ./dblog/log 配置 参考上文mongodb配置。 离线安装yapi在一台连接互联网的pc上安装node环境 在外网机器获取yapi源码并安装依赖使用git获取yapi源码, 如果没有git命令请按照对应平台的安装方法安装git. 创建一个新文件夹yapi, 使用clone将yapi源码放入vendors中: 123456$ mkdir yapi$ cd yapi$ git clone https://github.com/YMFE/yapi.git vendors$ cp vendors/config_example.json ./config.json$ cd vendors$ npm install --production 我这里还安装了pm2 1$ npm n install -S pm2 将创建的yapi文件夹打成压缩包得到yapi.tar.gz(其目录下有config.json和vendors) 1$ tar -czf yapi.tar.gz yapi 至此, 所有需要外部网络的操作已经完成, 可以进行内网部署. 启动yapi解压yapi.tar.gz 1$ tar -zxvf yapi.tar.gz 按需要修改yapi/config.json中的相关配置(例如管理员账号等) 初始化数据库: 12$ cd ./yapi/vendors$ npm run install-server 使用pm2启动 1$ npx n pm2 start ./server/app.js 启动完成后即可尝试访问yapi看是否成功, 具体地址要根据内网机器的ip和在config.json中配置的端口号 如果要关闭yapi服务, 可以使用 1$ npx n pm2 stop all 问题总结：两种方式安装yapi,按照正常方式安装都是无法安装的，有如下错误 方式1图形界面，yapi server 启动9090服务后，页面无法打开，会报错误，原因是无网络。方式2命令行安装，npm install –production 回报git错误，因需要联网git操作，原因无网络，npm使用私库代理也不行。 参考资料：https://yapi.ymfe.org/devops/index.html http://stlighter.github.io/2018/04/19/yapi%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2/ https://www.linuxidc.com/Linux/2018-01/150513.htm https://blog.csdn.net/luwei42768/article/details/78919073]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>部署</tag>
        <tag>yapi</tag>
        <tag>api</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vsftp部署]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F16%2Fvsftp%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[一、安装前的准备1、关闭防火墙或者开端口权限。一般是firewalld或者iptables。12$ systemctl stop firewalld$ systemctl disable firewalld 防火墙配置 1234567891011121314151617181920212223242526272829303132333435363738如果开启防火墙，请开放21端口，被动模式下设置最大和最小端口范围，并在防火墙开放端口范围。修改vsftpd.conf文件，添加pasv_min_port=8022 #最小端口pasv_max_port=8030 #最大端口pasv_promiscuous=YES #开启pasv（被动模式）放开21端口：firewall-cmd –zone=public –add-port=21/tcp –permanent放开8022-8030端口：firewall-cmd –zone=public –add-port=8022-8030/tcp –permanent重新加载防火墙：firewall-cmd –reload可能用到的命令：永久开放 ftp 服務：firewall-cmd –add-service=ftp –permanent (关闭ftp服务：firewall-cmd –remove-service=ftp –permanent) （验证不起作用）systemctl start firewalld 启动防火墙服务firewall-cmd –add-service=ftp 暂时开放ftp服务firewall-cmd –add-service=ftp –permanent永久开放ftp服務firewall-cmd –remove-service=ftp –permanent永久关闭ftp服務systemctl restart firewalld 重启firewalld服务firewall-cmd –reload 重载配置文件firewall-cmd –query-service ftp查看服务的启动状态firewall-cmd –list-all 显示防火墙应用列表firewall-cmd –add-port=8001/tcp 添加自定义的开放端口iptables -L -n | grep 21 查看设定是否生效firewall-cmd –state 检测防火墙状态 2、关闭sellinux12345678910# 临时关闭$ setenforce 0# 永久关闭$ vi /etc/selinux/config修改SELINUX=disabled# 查看是否关闭$ getenforce 二、安装vsftpd12345$ yum install -y vsftpd# 启动$ systemctl start vsftpd# 自启$ systemctl enable vsftpd 三、配置vsftpd创建vsftpd使用的系统用户，主目录为/home/vsftpd，禁止ssh登录。创建之后所有虚拟用户使用这个系统用户访问文件。useradd vsftpd -d /home/vsftpd -s /bin/false 方式一、虚拟用户配置1、创建虚拟用户主目录，比如虚拟用户叫ftp1，执行下面的命令。1$ mkdir -p /home/vsftpd/ftp1 2、创建这个虚拟用户123456$ vi /etc/vsftpd/loginusers.conf增加ftp1123456# 这样就创建了ftp1这个虚拟用户，密码为123456 3、根据这个文件创建数据库文件12$ db_load -T -t hash -f /etc/vsftpd/loginusers.conf /etc/vsftpd/loginusers.db$ chmod 600 /etc/vsftpd/loginusers.db 4、启用这个数据库文件12345$ vi /etc/pam.d/vsftpd注释掉所有内容后，增加下面的内容auth sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/loginusersaccount sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/loginusers 5、创建虚拟用户配置文件123456789$ mkdir /etc/vsftpd/userconf这里的文件名称必须与虚拟用户名一致$ vi /etc/vsftpd/userconf/ftp1增加下面的内容local_root=/home/vsftpd/ftp1/write_enable=YES# 设定主目录为/home/vsftpd/ftp1 6、最后修改主配置文件123456789101112131415$ vi /etc/vsftpd/vsftpd.conf更改anonymous_enable=NO# 去掉注释chroot_local_user=YESascii_upload_enable=YESascii_download_enable=YES# 增加guest_enable=YESguest_username=vsftpduser_config_dir=/etc/vsftpd/userconfallow_writeable_chroot=YES 配置介绍： 12345678910anonymous_enable=NO 禁止匿名用户登录chroot_local_user=YES 禁止用户访问除主目录以外的目录ascii_upload_enable=YES ascii_download_enable=YES 设定支持ASCII模式的上传和下载功能guest_enable=YES 启动虚拟用户guest_username=vsftpd 虚拟用户使用的系统用户名user_config_dir=/etc/vsftpd/userconf 虚拟用户使用的配置文件目录allow_writeable_chroot=YES 最新版的vsftpd为了安全必须用户主目录（也就是/home/vsftpd/ftp1）没有写权限，才能登录，或者使用allow_writeable_chroot=YES最后重启服务使配置生效systemctl restart vsftpd备注：设置ftp1目录权限为vsftpd, chown -R vsftpd:vsftpd /home/vsftpd/ftp1,否则没有权限创建目录等写的权限，设置777权限也不行。 方式二、本地用户配置配置 FTP 权限 1、了解 VSFTP 配置vsftpd 的配置目录为 /etc/vsftpd，包含下列的配置文件： vsftpd.conf 为主要配置文件 ftpusers 配置禁止访问 FTP 服务器的用户列表 user_list 配置用户访问控制——这里的用户默认情况（即在/etc/vsftpd/vsftpd.conf中设置了userlist_deny=YES）下也不能访问FTP服务器 2、阻止匿名访问和切换根目录匿名访问和切换根目录都会给服务器带来安全风险，我们把这两个功能关闭。编辑 /etc/vsftpd/vsftpd.conf，找到下面两处配置并修改： 123# 禁用匿名用户 YES 改为NO anonymous_enable=NO# 禁止切换根目录 删除# chroot_local_user=YES 3、修改默认根目录修改ftp的根目录只要修改/etc/vsftpd/vsftpd.conf文件即可： 加入如下几行： 123local_root=/var/www/htmlchroot_local_user=YESanon_root=/var/www/html 注：local_root 针对系统用户；anon_root 针对匿名用户。 编辑完成后保存配置，重新启动 FTP 服务 service vsftpd restart 其它配置项说明： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051anonymous_enable=YES #允许匿名登陆local_enable=YES #启动home目录write_enable=YES #ftp写的权限local_umask=022dirmessage_enable=YES #连接打印的消息connect_from_port_20=YES #20端口xferlog_std_format=YESidle_session_timeout=600data_connection_timeout=300accept_timeout=60connect_timeout=60ascii_upload_enable=YES #上传ascii_download_enable=YES #下载chroot_local_user=NO #是否限制用户在主目录活动chroot_list_enable=YES #启动限制用户的列表chroot_list_file=/etc/vsftpd/chroot_list #每行一个用户名allow_writeable_chroot=YES #允许写listen=NOlisten_ipv6=YESpasv_min_port=50000 允许ftp工具访问的端口起止端口pasv_max_port=60000pam_service_name=vsftpd #配置虚拟用户需要的userlist_enable=NO #配置yes之后，user_list的用户不能访问ftptcp_wrappers=YESchroot_list 文件需要自己建,内容一行一个用户名字anon_root=/data/ftp/public #修改匿名用户的访问路径 4、 创建 FTP 用户新建一个不能登录系统用户. 只用来登录ftp服务 ,这里如果没设置用户目录。默认是在home下： 1$ useradd ftpuser -d /home/vsftpd -s /bin/false 为ftpuser用户设置密码：passwd ftpuser 可能用到： 1234567设置用户的主目录：usermod -d /data/ftp ftpuser彻底删除用户：#userdel -rf Fuser //强制删除用户及相关目录文件变更用户属性：#usermod -s /sbin/nologinftpuser (/bin/bash：可以登录shell，/bin/false：禁止登录shell )查看当前服务：#netstat -lntp 备注：需要设置根目录权限为777 ，否则会出现无法写入的问题，chmod 777 /var/www/html 四、访问FTP通过 FTP 客户端工具访问 FTP 客户端工具众多，下面推荐两个常用的： WinSCP– Windows 下的 FTP 和 SFTP 连接客户端 FileZilla – 跨平台的 FTP 客户端，支持 Windows 和 Mac 本人测试时使用的是Xftp 打开Xftp软件，新建一个会话，输入对应的信息，点击确定(查看ip地址：ip addr) 选中我们新建的会话，点击连接 连接成功后就可以使用Xftp上传文件了 五、要使用Xshell连接，则需要安装openssh-service查看是否安装ssh安装包，CentOS是被访问者，所以需要安装ssh-server安装包（如果没任何输出显示表示没有安装 openssh-server，可以通过输入 yum install openssh-serve进行安装），查看命令为：rpm -qa | grep ssh，如下图所示，已经安装： 找到/etc/ssh目录下的sshd_config文件，修改一些参数。去掉端口和监听地址的注释；然后允许远程登录；再开启使用用户名密码作为连接验证开启sshd服务，service sshd start检查sshd是否开启，ps -e|grep sshd或者查看22端口是否被监听，netstat -an | grep 22使用Xshell进行连接，打开Xshell软件，新建一个会话，输入对应的信息，点击确定(查看ip地址：ip addr)选中我们新建的会话，点击连接连接成功后就可以使用Xshell执行命令了 参考：https://blog.csdn.net/will0532/article/details/79175478 https://blog.csdn.net/qq_32786873/article/details/78730303 https://www.cnblogs.com/huangye-dream/p/3454595.html https://blog.csdn.net/yifansj/article/details/72855484]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>vsftp</tag>
        <tag>ftp</tag>
        <tag>部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes创建资源对象yaml文件例子--pod]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F16%2Fkubernetes%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1yaml%E6%96%87%E4%BB%B6%E4%BE%8B%E5%AD%90-pod%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172apiVersion: v1 #指定api版本，此值必须在kubectl apiversion中,通过kubectl api-versions命令查询 kind: Pod #指定创建资源的角色/类型 metadata: #资源的元数据/属性 name: web04-pod #资源的名字，在同一个namespace中必须唯一 labels: #设定资源的标签，详情请见http://blog.csdn.net/liyingke112/article/details/77482384 k8s-app: apache version: v1 kubernetes.io/cluster-service: "true" annotations: #自定义注解列表 - name: String #自定义注解名字 spec:#specification of the resource content 指定该资源的内容 restartPolicy: Always #表明该容器一直运行，默认k8s的策略，在此容器退出后，会立即创建一个相同的容器 nodeSelector: #节点选择，先给主机打标签kubectl label nodes kube-node1 zone=node1 zone: node1 containers: - name: web04-pod #容器的名字 image: web:apache #容器使用的镜像地址 imagePullPolicy: Never #三个选择Always、Never、IfNotPresent，每次启动时检查和更新（从registery）images的策略， # Always，每次都检查 # Never，每次都不检查（不管本地是否有） # IfNotPresent，如果本地有就不检查，如果没有就拉取 command: ['sh'] #启动容器的运行命令，将覆盖容器中的Entrypoint,对应Dockefile中的ENTRYPOINT args: ["$(str)"] #启动容器的命令参数，对应Dockerfile中CMD参数 env: #指定容器中的环境变量 - name: str #变量的名字 value: "/etc/run.sh" #变量的值 resources: #资源管理，请求请见http://blog.csdn.net/liyingke112/article/details/77452630 requests: #容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行 cpu: 0.1 #CPU资源（核数），两种方式，浮点数或者是整数+m，0.1=100m，最少值为0.001核（1m） memory: 32Mi #内存使用量 limits: #资源限制 cpu: 0.5 memory: 32Mi ports: - containerPort: 80 #容器开发对外的端口 name: httpd #名称 protocol: TCP livenessProbe: #pod内容器健康检查的设置，详情请见http://blog.csdn.net/liyingke112/article/details/77531584 httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常 path: / #URI地址 port: 80 #host: 127.0.0.1 #主机地址 scheme: HTTP initialDelaySeconds: 180 #表明第一次检测在容器启动后多长时间后开始 timeoutSeconds: 5 #检测的超时时间 periodSeconds: 15 #检查间隔时间 #也可以用这种方法 #exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常 # command: # - cat # - /tmp/health #也可以用这种方法 #tcpSocket: //通过tcpSocket检查健康 # port: number lifecycle: #生命周期管理 postStart: #容器运行之前运行的任务 exec: command: - 'sh' - 'yum upgrade -y' preStop:#容器关闭之前运行的任务 exec: command: ['service httpd stop'] volumeMounts: #详情请见http://blog.csdn.net/liyingke112/article/details/76577520 - name: volume #挂载设备的名字，与volumes[*].name 需要对应 mountPath: /data #挂载到容器的某个路径下 readOnly: True volumes: #定义一组挂载设备 - name: volume #定义一个挂载设备的名字 #meptyDir: &#123;&#125; hostPath: path: /opt #挂载设备类型为hostPath，路径为宿主机下的/opt,这里设备类型支持很多种 转载：https://blog.csdn.net/liyingke112/article/details/76155428]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>yaml</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes创建资源对象yaml文件例子--rc]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F16%2Fkubernetes%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1yaml%E6%96%87%E4%BB%B6%E4%BE%8B%E5%AD%90-rc%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990apiVersion: v1 #指定api版本，此值必须在kubectl apiversion中kind: ReplicationController #指定创建资源的角色/类型metadata: #资源的元数据/属性 name: test-rc #资源的名字，在同一个namespace中必须唯一 labels: #设定资源的标签，详情请见http://blog.csdn.net/liyingke112/article/details/77482384 k8s-app: apache software: apache project: test app: test-rc version: v1 annotations: #自定义注解列表 - name: String #自定义注解名字spec: replicas: 2 #副本数量2 selector: #RC通过spec.selector来筛选要控制的Pod software: apache project: test app: test-rc version: v1 name: test-rc template: #这里Pod的定义 metadata: labels: #Pod的label，可以看到这个label与spec.selector相同 software: apache project: test app: test-rc version: v1 name: test-rc spec:#specification of the resource content 指定该资源的内容 restartPolicy: Always #表明该容器一直运行，默认k8s的策略，在此容器退出后，会立即创建一个相同的容器 nodeSelector: #节点选择，先给主机打标签kubectl label nodes kube-node1 zone=node1 zone: node1 containers: - name: web04-pod #容器的名字 image: web:apache #容器使用的镜像地址 imagePullPolicy: Never #三个选择Always、Never、IfNotPresent，每次启动时检查和更新（从registery）images的策略， # Always，每次都检查 # Never，每次都不检查（不管本地是否有） # IfNotPresent，如果本地有就不检查，如果没有就拉取 command: ['sh'] #启动容器的运行命令，将覆盖容器中的Entrypoint,对应Dockefile中的ENTRYPOINT args: ["$(str)"] #启动容器的命令参数，对应Dockerfile中CMD参数 env: #指定容器中的环境变量 - name: str #变量的名字 value: "/etc/run.sh" #变量的值 resources: #资源管理，请求请见http://blog.csdn.net/liyingke112/article/details/77452630 requests: #容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行 cpu: 0.1 #CPU资源（核数），两种方式，浮点数或者是整数+m，0.1=100m，最少值为0.001核（1m） memory: 32Mi #内存使用量 limits: #资源限制 cpu: 0.5 memory: 32Mi ports: - containerPort: 80 #容器开发对外的端口 name: httpd #名称 protocol: TCP livenessProbe: #pod内容器健康检查的设置，详情请见http://blog.csdn.net/liyingke112/article/details/77531584 httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常 path: / #URI地址 port: 80 #host: 127.0.0.1 #主机地址 scheme: HTTP initialDelaySeconds: 180 #表明第一次检测在容器启动后多长时间后开始 timeoutSeconds: 5 #检测的超时时间 periodSeconds: 15 #检查间隔时间 #也可以用这种方法 #exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常 # command: # - cat # - /tmp/health #也可以用这种方法 #tcpSocket: //通过tcpSocket检查健康 # port: number lifecycle: #生命周期管理 postStart: #容器运行之前运行的任务 exec: command: - 'sh' - 'yum upgrade -y' preStop:#容器关闭之前运行的任务 exec: command: ['service httpd stop'] volumeMounts: #详情请见http://blog.csdn.net/liyingke112/article/details/76577520 - name: volume #挂载设备的名字，与volumes[*].name 需要对应 mountPath: /data #挂载到容器的某个路径下 readOnly: True volumes: #定义一组挂载设备 - name: volume #定义一个挂载设备的名字 #meptyDir: &#123;&#125; hostPath: path: /opt #挂载设备类型为hostPath，路径为宿主机下的/opt,这里设备类型支持很多种]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>yaml</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批处理文件小技巧]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F16%2F%E6%89%B9%E5%A4%84%E7%90%86%E6%96%87%E4%BB%B6%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[批处理|多服务多窗口 123@echo off start cmd /k "d:&amp;&amp;cd d:\123&amp;&amp;echo 这是一个窗口&amp;&amp;pause&amp;&amp;ping 192.168.1.3&amp;&amp;ping 172.168.1.10" start cmd /k "d:&amp;&amp;cd d:\321&amp;&amp;echo 这是另一个窗口&amp;&amp;pause&amp;&amp;ping 192.168.1.20" #参数说明1、start 用来启动一个应用2、cmd /k 表示cmd后面的命令执行完后不关闭窗口。如果要在执行完成后关闭窗口可以用/c 。详细请使用cmd/?查看3、”命令1&amp;&amp;命令2&amp;&amp;..” 将要执行的多条命令使用引号全部包起来，并且在命令间用&amp;&amp;分隔。如果只有一条命令则不用引号也可以。 批处理|choice的使用示例示例1：12345678910111213141516171819@echo off ::设置CMD窗口字体颜色为0a 在CMD中输入命令 color /? 可查看颜色列表color 0a::设置CMD窗口显示模式为100列宽 20行高MODE con: COLS=100 LINES=20echo -------------------echo choice 命令示例echo -------------------echo.echo.:: /c按键列表 /m提示内容 /d默认选择 /t等待秒数 /d 必须和 /t同时出现choice /c abcde /m "请输入" /d e /t 5 ::用户选择的结果会按项目序号数字（从1开始）返回在errorlevel变量中if %errorlevel%==1 echo 你选择了aif %errorlevel%==2 echo 你选择了bif %errorlevel%==3 echo 你选择了cif %errorlevel%==4 echo 你选择了dif %errorlevel%==5 echo 你选择了e 示例2： 12345678910111213141516171819202122232425262728293031323334353637383940@echo offecho ***安装并启动mysql请输入1echo ***启动Tomcat请输入2echo *******************************************echo 备注：echo 1.通过关闭tomcat运行窗口关闭Tomcat###echo 2.以下为mysql启动及关闭操作###echo *******************************************echo ***启动mysql请输入3echo ***关闭mysql请输入4choice /C 1234 /m ""if %errorlevel%==1 goto installmysqlif %errorlevel%==2 goto starttomcatif %errorlevel%==3 goto startmysqlif %errorlevel%==4 goto stopmysql:installmysqlecho "设置Mysql环境变量"setx PATH "%PATH%;D:\iwhereEarth\mysql-5.6.39-winx64\bin" /mecho "设置Mysql环境变量成功"echo "安装MYSQL"pushd D:\iwhereEarth\mysql-5.6.39-winx64\binmysqld installnet start mysqlecho "Mysql安装并启动成功"goto end:starttomcatecho "启动Tomcat"start cmd /k "d:&amp;&amp;cd D:\iwhereEarth\apache-tomcat-7.0.63\bin&amp;&amp;echo Tomcat运行窗口&amp;&amp;catalina.bat run"goto end:startmysqlpushd D:\iwhereEarth\mysql-5.6.39-winx64\binnet start mysqlgoto end:stopmysqlpushd D:\iwhereEarth\mysql-5.6.39-winx64\binnet stop mysqlgoto end:endecho.&amp;pause 脚本设置环境变量12345678910111213::set system environment variable::set ant environment variablesetx ANT_HOME E:\tools\apache-ant-1.9.0 /msetx PATH "%PATH%;%ANT_HOME%\BIN" /m::set android environment variableSETX ANDROID_HOME E:\android\android-sdk-windows /mSETX PATH "%PATH;%ANDROID_HOME%\platform-tools" /mecho "设置成功"pauseexit]]></content>
      <categories>
        <category>Widows</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>小知识</tag>
        <tag>bat</tag>
        <tag>运维开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用工具及小技巧]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F16%2F%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E5%8F%8A%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[1、centos -Tab键命令补全 12345$ yum install -y bash-completion#执行脚本，使其生效或重新登录生效$ source /usr/share/bash-completion/bash_completion 2、自定义命令补全1234567# 自定义生成kubectl命令补全source &lt;(kubectl completion bash)# 将命令添加入bashrc文件，每次登录当前用户执行，使命令补全生效，也可添加入其它开机执行的脚本echo "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bashrc 3、ubuntu-Tab键命令补全编辑/etc/bash.bashrc 里面有这几行语句，去掉#注释 123456789101112131415#enable bash completion in interactive shellsif ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fifi 4、Linux设置环境变量在 linux 里设置环境变量的方法 （ export PATH ） 一般来说，配置交叉编译工具链的时候需要指定编译工具的路径，此时就需要设置环境变量。例如我的mips-linux-gcc编译器在“/opt/au1200_rm /build_tools/bin”目录下，build_tools就是我的编译工具，则有如下三种方法来设置环境变量： 4.1、直接用export命令：立即生效，重启丢失。 1export PATH=$PATH:/opt/au1200_rm/build_tools/bin 查看是否已经设好，可用命令export查看： 12345678#exportdeclare -x BASH_ENV=&quot;/root/.bashrc&quot;declare -x G_BROKEN_FILENAMES=&quot;1&quot;declare -x HISTSIZE=&quot;1000&quot;PATH里面已经有了我要加的编译器的路径。 4.2、修改profile文件：1234567$ vi /etc/profile在里面加入:export PATH=”$PATH:/opt/au1200_rm/build_tools/bin”$ . /etc/profile #执行命令使配置生效 4.3. 修改.bashrc文件：1234$ vi /root/.bashrc在里面加入：export PATH=”$PATH:/opt/au1200_rm/build_tools/bin” 后两种方法一般需要重新注销系统才能生效，最后可以通过echo命令测试一下： 12$ echo $PATH #输出变量看看输出里面是不是已经有了 /my_new_path这个路径了。 5、实时查看日志tail -f /var/log/messages 6、客户端(Xshell、SecureCRT)拖拉文件到服务器yum install lrzsz 7、YUM下载rpm包及依赖包#只下载bash-completion包到home目录，不进行安装yum install –downloadonly –downloaddir=/home bash-completion 8、删除多少前天备份#删除目录/mnt/backup_data下30天前后缀为.sql的文件 find /mnt/backup_data/ -name “*.sql” -type f -mtime +30 -exec rm -f {} \; 9、nmon显示系统性能显示工具123yum install nmonnmon 参考https://linux.cn/article-6467-1.html 10、Htop进程浏览器yum install htop 11、查看磁盘i/o工具 12345678910111213$ yum install sysstat#每2秒更新一次，-m 以MB显示，-k以kb显示iostat -d -m 2#oriostat -d -k 2#oriostat -d -m /dev/sda1 12345yum install iotop#c查看哪个进程占用i/oiotop 使用详解参考：http://man.linuxde.net/iotop 12、linux常用的监控命令12.1. top显示所有正在运行而且处于活动状态的实时进程， 而且会定期更新显示结果；它显示了CPU使用率，内存使用率，交换内存使用大小，调整缓存使用大小，缓冲区使用大小，进程PID， 使用的命令等信息。 12.2. vmstat 123456789101112131415161718192021222324252627一般是通过两个数字参数来完成的，第一个参数是采样时间间隔，单位是秒， 第二个参数是采样的次数r: 表示运行队列，如果队列过大说明CPU很繁忙，一般会造成CPU使用率高b: 表示阻塞的进程数swap: 虚拟内存已使用的大小，如果大于0，说明机器的物理内存不够了free: 空闲的物理内存大小buff: 系统占用的缓存大小（写缓存）cache： 直接用来记忆我们打开的文件，给文件做缓冲，读缓存si: 每秒从磁盘读入虚拟内存大小，如果这个值大于0，表示物理内存不足了so: 每秒虚拟内存写入磁盘的大小，如果这个值大于0， 表示物理内存不足了us: 用户cpu时间sy: 系统CPU时间， 如果值 太高，说明系统调用，例如是IO操作频繁id: 空闲CPU时间，一般来说 id + us + sy = 100wt: 等待IO的CPU时间 12.3. lsof列出打开的文件；它常用于以列表形式显示所有打开的文件和进程，包括磁盘文件，网络套接字，管道，设备和进程。 主要情形之一就是 无法挂载磁盘和显示正在使用或者打开某个文件的错误时，查看谁正在使用。 12.4. tcpdumpapt-get install tcpdump 用于捕捉或过滤网络上指定接口上接收或者传输的TCP/IP包。 -i : 网络接口 -c ： 需要输出包数量 12.5. netstat用于监控进出网络的包和网络接口统计的命令行工具，非常有用，用来监控网络性能，解决网络相关问题。 -h : 查看帮助 -r : 显示路由表 -i : 查看网络接口 12.6. Htop一个非常高级的交互式实时linux进程监控工具，和top相似，但更友好, 还支持鼠标。 sudo apt-get install htop 12.7. iotop监控linux磁盘I/O, 用于查找大量使用磁盘读写进程的时候。python版本需要2.7以上。 1$ apt-get install iotop -h: 查看帮助 12.8. iostat查看存储设备输入和输出状态统计的工具，用来追踪存储设备的性能 问题；包括设备，磁盘，NFS远程磁盘。 sudo apt-get install sysstat 123456789101112131415161718192021%user: 在用户级别运行所使用的CPU百分比%nice: 优先进程消耗的CPU时间，占所有CPU百分比%system: 在系统级别运行所使用的CPU百分比%iowait: cpu等待硬件I/O时，所占用的CPU百分比%steal: 管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比%idle: CPU空闲时间的百分比tps: 每秒发送到I/O的请求数KB_read/s: 每秒读取的block数KB_wrtn/s: 每秒写入的block数KB_read: 启动到现在block总数KB_wrtn: 启动到现在写入的block总数 12.9. iptraf用于采集通过网络接口的IP流量信息，包括tcp标记，icmp信息，TCP，UDP信等。 123$ sudo apt-get install iptraf$ sudo iptraf 12.10. nethogs监控每个进程使用的网络带宽 123$ sudo apt-get install nethogs$ sudo nethogs 12.11. iftop监控网络接口的应用网络带宽使用情况 123$ sudo apt-get install iftop$ sudo iftop 12345678910111213=&gt; : 表示 流量方向TX： 发送的流量RX： 接收的流量TOTAL： 总流量Cumm: 运行iftop到目前总流量peak: 流量峰会rates: 分别表示 过去2秒，10秒，40秒的平均流量 12.12. system monitor监控cpu,内存，进程，硬盘的信息；分为进程监控，资源监控，文件监控; 遗憾的是需要图形界面支持。 123sudo apt-get install gnome-system-monitorgnome-system-monitor 13、禁止用户登录系统1234567#禁止usermod -s /bin/false ftpuser#开启usermod -s /bin/base ftpuser 14、nmtui配置网卡使用nmtui命令（上一篇博客里有介绍界面）]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>小知识</tag>
        <tag>运维工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker小技巧]]></title>
    <url>%2Fkxinter.gitub.io%2F2018%2F08%2F15%2Fdocker%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[1、dockers开启privileged解决centos7容器无法使用systemctl命令的问题1docker run -d -e "container=docker" --privileged=true -v /sys/fs/cgroup:/sys/fs/cgroup --name centos7 centos:7.5 /usr/sbin/init]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>基础运维</tag>
        <tag>小知识</tag>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
